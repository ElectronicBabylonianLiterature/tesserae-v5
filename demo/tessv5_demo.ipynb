{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "tessv5_demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElectronicBabylonianLiterature/tesserae-v5/blob/master/demo/tessv5_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPoJ48OmfN-Z",
        "colab_type": "text"
      },
      "source": [
        "# Tesserae v5 Demo\n",
        "\n",
        "This demo will go over the basics of Tesserae v5 development up through February 5, 2019."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VebVIbB6sOy4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e625b0c1-09ce-40bb-f0c5-13972236fdab"
      },
      "source": [
        "!apt update\n",
        "!apt install -y mongodb\n",
        "!mkdir -p mongodb-data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Get:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Ign:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [214 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [41.2 kB]\n",
            "Get:15 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [93.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [869 kB]\n",
            "Get:18 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,845 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [102 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,302 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [13.6 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,406 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [89.0 kB]\n",
            "Get:24 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [890 kB]\n",
            "Get:25 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,003 kB]\n",
            "Get:26 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [9,282 B]\n",
            "Fetched 8,151 kB in 6s (1,255 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "77 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libpcap0.8 libstemmer0d libyaml-cpp0.5v5 mongo-tools mongodb-clients\n",
            "  mongodb-server mongodb-server-core\n",
            "The following NEW packages will be installed:\n",
            "  libpcap0.8 libstemmer0d libyaml-cpp0.5v5 mongo-tools mongodb mongodb-clients\n",
            "  mongodb-server mongodb-server-core\n",
            "0 upgraded, 8 newly installed, 0 to remove and 77 not upgraded.\n",
            "Need to get 53.1 MB of archives.\n",
            "After this operation, 215 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpcap0.8 amd64 1.8.1-6ubuntu1.18.04.1 [118 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libstemmer0d amd64 0+svn585-1build1 [62.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libyaml-cpp0.5v5 amd64 0.5.2-4ubuntu1 [150 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mongo-tools amd64 3.6.3-0ubuntu1 [12.3 MB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 mongodb-clients amd64 1:3.6.3-0ubuntu1.1 [20.2 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 mongodb-server-core amd64 1:3.6.3-0ubuntu1.1 [20.3 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 mongodb-server all 1:3.6.3-0ubuntu1.1 [12.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 mongodb amd64 1:3.6.3-0ubuntu1.1 [9,968 B]\n",
            "Fetched 53.1 MB in 7s (7,854 kB/s)\n",
            "Selecting previously unselected package libpcap0.8:amd64.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpcap0.8_1.8.1-6ubuntu1.18.04.1_amd64.deb ...\n",
            "Unpacking libpcap0.8:amd64 (1.8.1-6ubuntu1.18.04.1) ...\n",
            "Selecting previously unselected package libstemmer0d:amd64.\n",
            "Preparing to unpack .../1-libstemmer0d_0+svn585-1build1_amd64.deb ...\n",
            "Unpacking libstemmer0d:amd64 (0+svn585-1build1) ...\n",
            "Selecting previously unselected package libyaml-cpp0.5v5:amd64.\n",
            "Preparing to unpack .../2-libyaml-cpp0.5v5_0.5.2-4ubuntu1_amd64.deb ...\n",
            "Unpacking libyaml-cpp0.5v5:amd64 (0.5.2-4ubuntu1) ...\n",
            "Selecting previously unselected package mongo-tools.\n",
            "Preparing to unpack .../3-mongo-tools_3.6.3-0ubuntu1_amd64.deb ...\n",
            "Unpacking mongo-tools (3.6.3-0ubuntu1) ...\n",
            "Selecting previously unselected package mongodb-clients.\n",
            "Preparing to unpack .../4-mongodb-clients_1%3a3.6.3-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking mongodb-clients (1:3.6.3-0ubuntu1.1) ...\n",
            "Selecting previously unselected package mongodb-server-core.\n",
            "Preparing to unpack .../5-mongodb-server-core_1%3a3.6.3-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking mongodb-server-core (1:3.6.3-0ubuntu1.1) ...\n",
            "Selecting previously unselected package mongodb-server.\n",
            "Preparing to unpack .../6-mongodb-server_1%3a3.6.3-0ubuntu1.1_all.deb ...\n",
            "Unpacking mongodb-server (1:3.6.3-0ubuntu1.1) ...\n",
            "Selecting previously unselected package mongodb.\n",
            "Preparing to unpack .../7-mongodb_1%3a3.6.3-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking mongodb (1:3.6.3-0ubuntu1.1) ...\n",
            "Setting up libstemmer0d:amd64 (0+svn585-1build1) ...\n",
            "Setting up libyaml-cpp0.5v5:amd64 (0.5.2-4ubuntu1) ...\n",
            "Setting up mongodb-server-core (1:3.6.3-0ubuntu1.1) ...\n",
            "Setting up libpcap0.8:amd64 (1.8.1-6ubuntu1.18.04.1) ...\n",
            "Setting up mongodb-clients (1:3.6.3-0ubuntu1.1) ...\n",
            "Setting up mongodb-server (1:3.6.3-0ubuntu1.1) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/mongodb.service → /lib/systemd/system/mongodb.service.\n",
            "Setting up mongo-tools (3.6.3-0ubuntu1) ...\n",
            "Setting up mongodb (1:3.6.3-0ubuntu1.1) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.41) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYNtsiNrsjCr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7a7feb03-7e1b-46b1-d595-f76c70655a8e"
      },
      "source": [
        "!mongod --dbpath ./mongodb-data --fork --logpath ./mongod.log"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "about to fork child process, waiting until server is ready for connections.\n",
            "forked process: 1326\n",
            "child process started successfully, parent exiting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTHNox59fPef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "291768f0-0cc8-44e8-d54c-b36acdf94dbf"
      },
      "source": [
        "!pip install git+https://github.com/ElectronicBabylonianLiterature/tesserae-v5"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ElectronicBabylonianLiterature/tesserae-v5\n",
            "  Cloning https://github.com/ElectronicBabylonianLiterature/tesserae-v5 to /tmp/pip-req-build-11yzricd\n",
            "  Running command git clone -q https://github.com/ElectronicBabylonianLiterature/tesserae-v5 /tmp/pip-req-build-11yzricd\n",
            "Collecting cltk>=0.1.83\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/20/42060504debda1fe85808d32ae82344e2efa4343dda15ec0151cf6bfe13d/cltk-0.1.121.tar.gz (625kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.5 in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (3.2.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (1.18.5)\n",
            "Requirement already satisfied: pymongo>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (3.10.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (4.41.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (5.5.0)\n",
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/f9/c315aa88e51fabdc08e91b333cfefb255aff04a2ee96d632c32cb19180c9/GitPython-3.1.3-py3-none-any.whl (451kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 14.1MB/s \n",
            "\u001b[?25hCollecting python-crfsuite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 10.1MB/s \n",
            "\u001b[?25hCollecting pyuca\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/88/aeeee34d88f841aca712a8c18fbd62a33eaad8f2dbe535e87f3c829b02f9/pyuca-1.2-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 20.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from cltk>=0.1.83->tesserae==0.1a1) (3.13)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from cltk>=0.1.83->tesserae==0.1a1) (2019.12.20)\n",
            "Collecting whoosh\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/19/24d0f1f454a2c1eb689ca28d2f178db81e5024f42d82729a4ff6771155cf/Whoosh-2.7.4-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 23.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.5->tesserae==0.1a1) (1.12.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.4MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: tesserae, cltk\n",
            "  Building wheel for tesserae (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tesserae: filename=tesserae-0.1a1-cp36-none-any.whl size=61754 sha256=af923ff2359d5a2f3f5ebc33ef71271abcd9e1fa12977708d08197292927b5e2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-41f6zcr_/wheels/15/12/9b/1a9461e40ceddee12498dee0f289daa405301c5f616adf3a1c\n",
            "  Building wheel for cltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cltk: filename=cltk-0.1.121-cp36-none-any.whl size=711645 sha256=a93164ef61977907ce2f8d1dad0a41964004aa64a3934b4aec00365e941b6051\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/c9/6b/e60acb6f511ebe008f3e961e894d57598517b25c4cbffbb70f\n",
            "Successfully built tesserae cltk\n",
            "Installing collected packages: smmap, gitdb, gitpython, python-crfsuite, pyuca, whoosh, cltk, tesserae\n",
            "Successfully installed cltk-0.1.121 gitdb-4.0.5 gitpython-3.1.3 python-crfsuite-0.9.7 pyuca-1.2 smmap-3.0.4 tesserae-0.1a1 whoosh-2.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIR-RVZbfN-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2895c039-253d-4d61-f53a-52dd074fc52b"
      },
      "source": [
        "import json\n",
        "\n",
        "from tesserae.db import TessMongoConnection\n",
        "from tesserae.db.entities import Match, Text, Token, Unit\n",
        "from tesserae.utils import TessFile\n",
        "from tesserae.tokenizers import GreekTokenizer, LatinTokenizer\n",
        "from tesserae.unitizer import Unitizer\n",
        "from tesserae.matchers import DefaultMatcher\n",
        "from tesserae.matchers.sparse_encoding import SparseMatrixSearch\n",
        "\n",
        "# Set up the connection and clean up the database\n",
        "connection = TessMongoConnection('127.0.0.1', 27017, None, None, 'tesstest')\n",
        "\n",
        "# Clean up the previous demo\n",
        "connection.connection['feature_sets'].delete_many({})\n",
        "connection.connection['feature']\n",
        "connection.connection['frequencies'].delete_many({})\n",
        "connection.connection['matches'].delete_many({})\n",
        "connection.connection['match_sets'].delete_many({})\n",
        "connection.connection['texts'].delete_many({})\n",
        "connection.connection['tokens'].delete_many({})\n",
        "connection.connection['units'].delete_many({})"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pymongo.results.DeleteResult at 0x7f26e722a748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noYHkyCrfN-f",
        "colab_type": "text"
      },
      "source": [
        "## Loading and Storing New Texts\n",
        "\n",
        "The Tesserae database catalogs metadata, including the title, author, and year published, as well as integrity information like filepath, MD5 hash, and CTS URN.\n",
        "\n",
        "We start by loading in some metadata from `text_metadata.json`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI0kgCX7SVRn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0ca5aa3-a4ba-49d1-bf54-7eca7b03f536"
      },
      "source": [
        "!mkdir -p grc\n",
        "!mkdir -p la\n",
        "\n",
        "!wget https://raw.githubusercontent.com/ElectronicBabylonianLiterature/tesserae-v5/master/demo/text_metadata.json -O ./text_metadata.json -nc\n",
        "\n",
        "!wget https://raw.githubusercontent.com/ElectronicBabylonianLiterature/tesserae-v5/master/demo/grc/euripides.heracles.tess -O ./grc/euripides.heracles.tess -nc\n",
        "!wget https://raw.githubusercontent.com/ElectronicBabylonianLiterature/tesserae-v5/master/demo/grc/plato.epistles.tess -O ./grc/plato.epistles.tess -nc\n",
        "!wget https://raw.githubusercontent.com/ElectronicBabylonianLiterature/tesserae-v5/master/demo/la/cicero.de_oratore.tess -O ./la/cicero.de_oratore.tess -nc\n",
        "!wget https://raw.githubusercontent.com/ElectronicBabylonianLiterature/tesserae-v5/master/demo/la/vergil.aeneid.tess -O ./la/vergil.aeneid.tess -nc\n",
        "\n",
        "!ls -Rl"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘./text_metadata.json’ already there; not retrieving.\n",
            "File ‘./grc/euripides.heracles.tess’ already there; not retrieving.\n",
            "File ‘./grc/plato.epistles.tess’ already there; not retrieving.\n",
            "File ‘./la/cicero.de_oratore.tess’ already there; not retrieving.\n",
            "File ‘./la/vergil.aeneid.tess’ already there; not retrieving.\n",
            ".:\n",
            "total 28\n",
            "drwxr-xr-x 2 root root 4096 Jul  8 12:14 grc\n",
            "drwxr-xr-x 2 root root 4096 Jul  8 12:14 la\n",
            "drwxr-xr-x 4 root root 4096 Jul  8 12:17 mongodb-data\n",
            "-rw-r--r-- 1 root root 4110 Jul  8 12:17 mongod.log\n",
            "drwxr-xr-x 1 root root 4096 Jul  8 12:13 sample_data\n",
            "-rw-r--r-- 1 root root 1185 Jul  8 12:14 text_metadata.json\n",
            "\n",
            "./grc:\n",
            "total 368\n",
            "-rw-r--r-- 1 root root 132190 Jul  8 12:14 euripides.heracles.tess\n",
            "-rw-r--r-- 1 root root 240953 Jul  8 12:14 plato.epistles.tess\n",
            "\n",
            "./la:\n",
            "total 1048\n",
            "-rw-r--r-- 1 root root 435279 Jul  8 12:14 cicero.de_oratore.tess\n",
            "-rw-r--r-- 1 root root 633564 Jul  8 12:14 vergil.aeneid.tess\n",
            "\n",
            "./mongodb-data:\n",
            "total 152\n",
            "-rw------- 1 root root 16384 Jul  8 12:17 collection-0--7003481119611752176.wt\n",
            "-rw------- 1 root root 16384 Jul  8 12:17 collection-2--7003481119611752176.wt\n",
            "drwx------ 2 root root  4096 Jul  8 12:17 diagnostic.data\n",
            "-rw------- 1 root root 16384 Jul  8 12:17 index-1--7003481119611752176.wt\n",
            "-rw------- 1 root root 16384 Jul  8 12:17 index-3--7003481119611752176.wt\n",
            "drwx------ 2 root root  4096 Jul  8 12:16 journal\n",
            "-rw------- 1 root root 16384 Jul  8 12:17 _mdb_catalog.wt\n",
            "-rw------- 1 root root     5 Jul  8 12:16 mongod.lock\n",
            "-rw------- 1 root root 16384 Jul  8 12:17 sizeStorer.wt\n",
            "-rw------- 1 root root   114 Jul  8 12:16 storage.bson\n",
            "-rw------- 1 root root    48 Jul  8 12:16 WiredTiger\n",
            "-rw------- 1 root root  4096 Jul  8 12:16 WiredTigerLAS.wt\n",
            "-rw------- 1 root root    21 Jul  8 12:16 WiredTiger.lock\n",
            "-rw------- 1 root root  1036 Jul  8 12:17 WiredTiger.turtle\n",
            "-rw------- 1 root root 24576 Jul  8 12:17 WiredTiger.wt\n",
            "\n",
            "./mongodb-data/diagnostic.data:\n",
            "total 24\n",
            "-rw------- 1 root root 13019 Jul  8 12:17 metrics.2020-07-08T12-16-01Z-00000\n",
            "-rw------- 1 root root  7836 Jul  8 12:17 metrics.interim\n",
            "\n",
            "./mongodb-data/journal:\n",
            "total 307200\n",
            "-rw------- 1 root root 104857600 Jul  8 12:17 WiredTigerLog.0000000001\n",
            "-rw------- 1 root root 104857600 Jul  8 12:16 WiredTigerPreplog.0000000001\n",
            "-rw------- 1 root root 104857600 Jul  8 12:16 WiredTigerPreplog.0000000002\n",
            "\n",
            "./sample_data:\n",
            "total 55500\n",
            "-rw-r--r-- 1 root root   301141 Jun 26 16:26 california_housing_test.csv\n",
            "-rw-r--r-- 1 root root  1706430 Jun 26 16:26 california_housing_train.csv\n",
            "-rw-r--r-- 1 root root 18289443 Jun 26 16:26 mnist_test.csv\n",
            "-rw-r--r-- 1 root root 36523880 Jun 26 16:26 mnist_train_small.csv\n",
            "-rwxr-xr-x 1 root root      930 Jan  1  2000 README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNGacqEVfN-g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4d561e79-a49d-48e1-8fdf-43d4d2aaeb52"
      },
      "source": [
        "with open('text_metadata.json', 'r') as f:\n",
        "    text_meta = json.load(f)\n",
        "\n",
        "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
        "print('{}{}{}{}'.format('-----'.ljust(15), '------'.ljust(15), '--------'.ljust(15), '----'))\n",
        "for t in text_meta:\n",
        "    print('{}{}{}{}'.format(t['title'].ljust(15), t['author'].ljust(15), t['language'].ljust(15), str(t['year']).ljust(15)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title          Author         Language       Year\n",
            "-----          ------         --------       ----\n",
            "aeneid         vergil         latin          19             \n",
            "de oratore     cicero         latin          38             \n",
            "heracles       euripides      greek          -416           \n",
            "epistles       plato          greek          -280           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGn9rNDmfN-j",
        "colab_type": "text"
      },
      "source": [
        "Then insert the new texts with `TessMongoConnection.insert` after converting the raw JSON to Tesserae `Text` entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEjoXuPcfN-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "41aad112-b002-45d7-f5c3-36c073f67452"
      },
      "source": [
        "texts = []\n",
        "for t in text_meta:\n",
        "    texts.append(Text.json_decode(t))\n",
        "result = connection.insert(texts)\n",
        "print('Inserted {} texts.'.format(len(result.inserted_ids)))\n",
        "print(result.inserted_ids)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inserted 4 texts.\n",
            "[ObjectId('5f05b94b99088a98103610ad'), ObjectId('5f05b94b99088a98103610ae'), ObjectId('5f05b94b99088a98103610af'), ObjectId('5f05b94b99088a98103610b0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MUtvQfXfN-p",
        "colab_type": "text"
      },
      "source": [
        "We can retrieve the inserted texts with `TessMongoConnection.find`. These texts will be converted to objects representing the database entries. The returned text list can be filtered by any valid field in the text database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6S1JGudfN-q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "de34a9d4-cbc1-4dfa-ebea-80f6539ccf1a"
      },
      "source": [
        "texts = connection.find('texts', _id=result.inserted_ids)\n",
        "\n",
        "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
        "for t in texts:\n",
        "    print('{}{}{}{}'.format(t.title.ljust(15), t.author.ljust(15), t.language.ljust(15), t.year))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title          Author         Language       Year\n",
            "aeneid         vergil         latin          19\n",
            "de oratore     cicero         latin          38\n",
            "heracles       euripides      greek          -416\n",
            "epistles       plato          greek          -280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U4h9b7RfN-s",
        "colab_type": "text"
      },
      "source": [
        "## Loading .tess Files\n",
        "\n",
        "Text metadata includes the path to the .tess file on the local filesystem. Using a Text retrieved from the database, the file can be loaded for further processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jc_ruUyfN-s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8977086c-5a62-42a9-f15c-88518016f453"
      },
      "source": [
        "tessfile = TessFile(texts[0].path, metadata=texts[0])\n",
        "\n",
        "print(tessfile.path)\n",
        "print(len(tessfile))\n",
        "print(tessfile[270])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "la/vergil.aeneid.tess\n",
            "9908\n",
            "<verg. aen. 1.271>\ttransferet, et longam multa vi muniet Albam.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyU-fhZmfN-v",
        "colab_type": "text"
      },
      "source": [
        "We can iterate through the file line-by-line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT5jDyUTfN-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "948384f8-97c5-4619-89a0-a6d6a736147b"
      },
      "source": [
        "lines = tessfile.readlines()\n",
        "for i in range(10):\n",
        "    print(next(lines))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<verg. aen. 1.1>\tArma virumque cano, Troiae qui primus ab oris\n",
            "\n",
            "<verg. aen. 1.2>\tItaliam, fato profugus, Laviniaque venit\n",
            "\n",
            "<verg. aen. 1.3>\tlitora, multum ille et terris iactatus et alto\n",
            "\n",
            "<verg. aen. 1.4>\tvi superum saevae memorem Iunonis ob iram;\n",
            "\n",
            "<verg. aen. 1.5>\tmulta quoque et bello passus, dum conderet urbem,\n",
            "\n",
            "<verg. aen. 1.6>\tinferretque deos Latio, genus unde Latinum,\n",
            "\n",
            "<verg. aen. 1.7>\tAlbanique patres, atque altae moenia Romae.\n",
            "\n",
            "<verg. aen. 1.8>\tMusa, mihi causas memora, quo numine laeso,\n",
            "\n",
            "<verg. aen. 1.9>\tquidve dolens, regina deum tot volvere casus\n",
            "\n",
            "<verg. aen. 1.10>\tinsignem pietate virum, tot adire labores\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehzPA8bfN-y",
        "colab_type": "text"
      },
      "source": [
        "We can also iterate token-by-token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMlabOnkfN-y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e99d32e9-f799-4d00-c0a2-47c193264650"
      },
      "source": [
        "tokens = tessfile.read_tokens()\n",
        "for i in range(10):\n",
        "    print(next(tokens))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arma\n",
            "virumque\n",
            "cano,\n",
            "Troiae\n",
            "qui\n",
            "primus\n",
            "ab\n",
            "oris\n",
            "Italiam,\n",
            "fato\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3skP5flfN-1",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing a Text\n",
        "\n",
        "Texts can be tokenized with `tesserae.tokenizers` objects. These objects are designed to normalize and compute features for tokens of a specific language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08wu-WZrfN-1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "fcbf639f-6b1a-41ff-ff81-fb702aafa77b"
      },
      "source": [
        "tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
        "\n",
        "tokens, tags, features = tokenizer.tokenize(tessfile.read(), text=tessfile.metadata)\n",
        "\n",
        "print(len(tokens), len(tags), len(features))\n",
        "\n",
        "print('{}{}{}{}'.format('Raw'.ljust(15), 'Normalized'.ljust(15), 'Lemmata'.ljust(20), 'Frequency'))\n",
        "print('{}{}{}{}'.format('---'.ljust(15), '----------'.ljust(15), '-------'.ljust(20), '---------'))\n",
        "for i in range(20):\n",
        "    if len(tokens[i].features):\n",
        "        print('{}{}{}{}'.format(tokens[i].display.ljust(15),\n",
        "                              str(tokens[i].features['form'].token).ljust(20),\n",
        "                              str(tokens[i].features['lemmata'][0].token).ljust(20),\n",
        "                              list(tokens[i].features['form'].frequencies.values())[0]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "127526 9896 23756\n",
            "Raw            Normalized     Lemmata             Frequency\n",
            "---            ----------     -------             ---------\n",
            "Arma           arma                arma                148\n",
            "virumque       uirumque            uir                 4\n",
            "cano           cano                canus               3\n",
            "Troiae         troiae              troius              36\n",
            "qui            qui                 quis                166\n",
            "primus         primus              primus              34\n",
            "ab             ab                  ab                  136\n",
            "oris           oris                ora                 32\n",
            "Italiam        italiam             italia              32\n",
            "fato           fato                for                 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0k_igU5fN-4",
        "colab_type": "text"
      },
      "source": [
        "Processed tokens can then be stored in and retrieved from the database, similar to text metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urZTBdFzfN-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "384f0330-c415-4f91-abad-372d3b724479"
      },
      "source": [
        "result = connection.insert(features)\n",
        "print('Inserted {} feature entities out of {}'.format(len(result.inserted_ids), len(features)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inserted 23756 feature entities out of 23756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuLJFTw2fN-6",
        "colab_type": "text"
      },
      "source": [
        "## Unitizing a Text\n",
        "\n",
        "Texts can be unitized into lines and phrases, and the intertext matches are found between units of text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oOUmwI7fN-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "8de91d03-87de-4b2a-ad77-a14e438e74dc"
      },
      "source": [
        "# Unitizing lines of a poem\n",
        "unitizer = Unitizer()\n",
        "lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
        "\n",
        "print('Lines\\n-----')\n",
        "for line in lines[:20]:\n",
        "        print(''.join([str(line.tags), ': '] + [t['display'] for t in line.tokens]))\n",
        "        \n",
        "print('\\n\\nPhrases\\n-------')\n",
        "for phrase in phrases[:20]:\n",
        "        print(''.join([str(phrase.tags), ': '] + [t['display'] for t in phrase.tokens]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lines\n",
            "-----\n",
            "['1.1']: ArmavirumquecanoTroiaequiprimusaboris\n",
            "['1.2']: ItaliamfatoprofugusLaviniaquevenit\n",
            "['1.3']: litoramultumilleetterrisiactatusetalto\n",
            "['1.4']: visuperumsaevaememoremIunonisobiram\n",
            "['1.5']: multaquoqueetbellopassusdumcondereturbem\n",
            "['1.6']: inferretquedeosLatiogenusundeLatinum\n",
            "['1.7']: AlbaniquepatresatquealtaemoeniaRomae\n",
            "['1.8']: Musamihicausasmemoraquonuminelaeso\n",
            "['1.9']: quidvedolensreginadeumtotvolverecasus\n",
            "['1.10']: insignempietatevirumtotadirelabores\n",
            "['1.11']: impuleritTantaeneanimiscaelestibusirae\n",
            "['1.12']: UrbsantiquafuitTyriitenuerecoloni\n",
            "['1.13']: KarthagoItaliamcontraTiberinaquelonge\n",
            "['1.14']: ostiadivesopumstudiisqueasperrimabelli\n",
            "['1.15']: quamIunoferturterrismagisomnibusunam\n",
            "['1.16']: posthabitacoluisseSamohicilliusarma\n",
            "['1.17']: hiccurrusfuithocregnumdeagentibusesse\n",
            "['1.18']: siquafatasinantiamtumtenditquefovetque\n",
            "['1.19']: ProgeniemsedenimTroianoasanguineduci\n",
            "['1.20']: audieratTyriasolimquaeverteretarces\n",
            "\n",
            "\n",
            "Phrases\n",
            "-------\n",
            "['1.1', '1.2', '1.3', '1.4']: ArmavirumquecanoTroiaequiprimusaborisItaliamfatoprofugusLaviniaquevenitlitoramultumilleetterrisiactatusetaltovisuperumsaevaememoremIunonisobiram\n",
            "['1.5', '1.6', '1.7']: multaquoqueetbellopassusdumcondereturbeminferretquedeosLatiogenusundeLatinumAlbaniquepatresatquealtaemoeniaRomae\n",
            "['1.8', '1.9', '1.10', '1.11']: Musamihicausasmemoraquonuminelaesoquidvedolensreginadeumtotvolverecasusinsignempietatevirumtotadirelaboresimpulerit\n",
            "['1.11']: Tantaeneanimiscaelestibusirae\n",
            "['1.12', '1.13', '1.14']: UrbsantiquafuitTyriitenuerecoloniKarthagoItaliamcontraTiberinaquelongeostiadivesopumstudiisqueasperrimabelli\n",
            "['1.15', '1.16']: quamIunoferturterrismagisomnibusunamposthabitacoluisseSamo\n",
            "['1.16', '1.17']: hicilliusarmahiccurrusfuit\n",
            "['1.17', '1.18']: hocregnumdeagentibusessesiquafatasinantiamtumtenditquefovetque\n",
            "['1.19', '1.20']: ProgeniemsedenimTroianoasanguineduciaudieratTyriasolimquaeverteretarces\n",
            "['1.21', '1.22']: hincpopulumlateregembelloquesuperbumventurumexcidioLibyae\n",
            "['1.22']: sicvolvereParcas\n",
            "['1.23', '1.24', '1.25', '1.26']: IdmetuensveterisquememorSaturniabelliprimaquodadTroiamprocarisgesseratArgisnecdumetiamcausaeirarumsaeviquedoloresexciderantanimo\n",
            "['1.26', '1.27', '1.28']: manetaltamenterepostumiudiciumParidisspretaequeiniuriaformaeetgenusinvisumetraptiGanymedishonores\n",
            "['1.29', '1.30', '1.31', '1.32']: HisaccensasuperiactatosaequoretotoTroasreliquiasDanaumatqueimmitisAchilliarcebatlongeLatiomultosqueperannoserrabantactifatismariaomniacircum\n",
            "['1.33']: TantaemoliseratRomanamconderegentem\n",
            "['1.34', '1.35', '1.36', '1.37']: VixeconspectuSiculaetellurisinaltumveladabantlaetietspumassalisaereruebantcumIunoaeternumservanssubpectorevolnushaecsecum\n",
            "['1.37', '1.38']: MeneinceptodesisterevictamnecposseItaliaTeucrorumavertereregem\n",
            "['1.39']: Quippevetorfatis\n",
            "['1.39', '1.40', '1.41']: PallasneexurereclassemArgivumatqueipsospotuitsubmergerepontouniusobnoxametfuriasAiacisOilei\n",
            "['1.42', '1.43', '1.44', '1.45']: IpsaIovisrapidumiaculataenubibusignemdisiecitqueratesevertitqueaequoraventisillumexpirantemtransfixopectoreflammasturbinecorripuitscopuloqueinfixitacuto\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwKa94FEfN-9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4c9fcd09-0194-4d35-c240-071c03e2946b"
      },
      "source": [
        "# Unitizing phrases of a poem or prose\n",
        "result = connection.insert(lines + phrases)\n",
        "print('Inserted {} units out of {}.'.format(len(result.inserted_ids), len(lines + phrases)))\n",
        "\n",
        "\n",
        "result = connection.insert(tokens)\n",
        "print('Inserted {} tokens out of {}.'.format(len(result.inserted_ids), len(tokens)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inserted 14983 units out of 14983.\n",
            "Inserted 127526 tokens out of 127526.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOdy-Up5fN-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "4e1a0803-0bc0-4ca0-d25a-502ee00e87bb"
      },
      "source": [
        "for text in texts[1:]:\n",
        "    tessfile = TessFile(text.path, metadata=text)\n",
        "    tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
        "\n",
        "    \n",
        "    tokens, tags, frequencies, feature_sets = tokenizer.tokenize(tessfile.read(), text=tessfile.metadata)\n",
        "        \n",
        "    tokens = tokenizer.tokens\n",
        "    result = connection.insert(feature_sets)\n",
        "    result = connection.insert(frequencies)\n",
        "    \n",
        "    unitizer = Unitizer()\n",
        "    lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
        "    result = connection.insert(lines + phrases)\n",
        "    \n",
        "    result = connection.insert(tokens)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e96d43cd46cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtessfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtessfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi09bzDPfN_C",
        "colab_type": "text"
      },
      "source": [
        "## Matching\n",
        "\n",
        "Once the Texts, Tokens, and Units are in the database, we can then find intertext matches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEJyHxDCfN_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "matcher = DefaultMatcher(connection)\n",
        "match_texts = [t for t in texts if t.language == 'greek']\n",
        "\n",
        "start = time.time()\n",
        "matches, match_set = matcher.match(match_texts, 'phrase', 'form', distance_metric='span', stopwords=20, max_distance=10)\n",
        "print(\"Completed matching in {0:.2f}s\".format(time.time() - start))\n",
        "\n",
        "matches.sort(key=lambda x: x.score, reverse=True)\n",
        "\n",
        "# result = connection.insert(match_set)\n",
        "# print('Inserted {} match set entities out of {}'.format(len(result.inserted_ids), 1))\n",
        "result = connection.insert(matches)\n",
        "print('Inserted {} match entities out of {}'.format(len(result.inserted_ids), len(matches)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzLx0OqgfN_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matches = connection.aggregate('matches', [\n",
        "    {'$match': {'match_set': match_set.id}},\n",
        "    {'$sort': {'score': -1}},\n",
        "    {'$limit': 20},\n",
        "    {'$lookup': {\n",
        "        'from': 'units',\n",
        "        'let': {'m_units': '$units'},\n",
        "        'pipeline': [\n",
        "            {'$match': {'$expr': {'$in': ['$_id', '$$m_units']}}},\n",
        "            {'$lookup': {\n",
        "                'from': 'tokens',\n",
        "                'localField': '_id',\n",
        "                'foreignField': 'phrase',\n",
        "                'as': 'tokens'\n",
        "            }},\n",
        "            {'$sort': {'index': 1}}\n",
        "        ],\n",
        "        'as': 'units'\n",
        "    }},\n",
        "    {'$lookup': {\n",
        "        'from': 'tokens',\n",
        "        'localField': 'tokens',\n",
        "        'foreignField': '_id',\n",
        "        'as': 'tokens'\n",
        "    }},\n",
        "    {'$project': {\n",
        "        'units': True,\n",
        "        'score': True,\n",
        "        'tokens': '$tokens.feature_set'\n",
        "    }},\n",
        "    {'$lookup': {\n",
        "        'from': 'feature_sets',\n",
        "        'localField': 'tokens',\n",
        "        'foreignField': '_id',\n",
        "        'as': 'tokens'\n",
        "    }}\n",
        "])\n",
        "\n",
        "print('\\n')\n",
        "print('{}{}'.format('Score'.ljust(15), 'Match Tokens'.ljust(15)))\n",
        "print('{}{}'.format('-----'.ljust(15), '------------'.ljust(15)))\n",
        "for m in matches:\n",
        "    print('{}{}'.format(('%.3f'%(m.score)).ljust(15), ', '.join(list(set([t['form'] for t in m.tokens])))))\n",
        "    print('{} {} {}: {}'.format(match_texts[0].author, match_texts[0].title, m.units[0]['tags'], ''.join([t['display'] for t in m.units[0]['tokens']])))\n",
        "    print('{} {} {}: {}'.format(match_texts[1].author, match_texts[1].title, m.units[1]['tags'], ''.join([t['display'] for t in m.units[1]['tokens']])))\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1NjY6B7fN_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}