{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesserae v5 Demo\n",
    "\n",
    "This demo will go over the basics of Tesserae v5 development up through February 5, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult at 0x7f9a96fb9f88>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from tesserae.db import TessMongoConnection\n",
    "from tesserae.db.entities import Frequency, Match, Text, Token, Unit\n",
    "from tesserae.utils import TessFile\n",
    "from tesserae.tokenizers import GreekTokenizer, LatinTokenizer\n",
    "from tesserae.unitizer import Unitizer\n",
    "from tesserae.matchers import AggregationMatcher\n",
    "from tesserae.matchers.sparse_encoding import SparseMatrixSearch\n",
    "\n",
    "# Set up the connection and clean up the database\n",
    "connection = TessMongoConnection('127.0.0.1', 27017, None, None, 'tesstest')\n",
    "\n",
    "# Clean up the previous demo\n",
    "connection.connection['feature_sets'].delete_many({})\n",
    "connection.connection['feature']\n",
    "connection.connection['frequencies'].delete_many({})\n",
    "connection.connection['matches'].delete_many({})\n",
    "connection.connection['match_sets'].delete_many({})\n",
    "connection.connection['texts'].delete_many({})\n",
    "connection.connection['tokens'].delete_many({})\n",
    "connection.connection['units'].delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Storing New Texts\n",
    "\n",
    "The Tesserae database catalogs metadata, including the title, author, and year published, as well as integrity information like filepath, MD5 hash, and CTS URN.\n",
    "\n",
    "We start by loading in some metadata from `text_metadata.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title          Author         Language       Year\n",
      "-----          ------         --------       ----\n",
      "aeneid         vergil         latin          19             \n",
      "de oratore     cicero         latin          38             \n",
      "heracles       euripides      greek          -416           \n",
      "epistles       plato          greek          -280           \n"
     ]
    }
   ],
   "source": [
    "with open('text_metadata.json', 'r') as f:\n",
    "    text_meta = json.load(f)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "print('{}{}{}{}'.format('-----'.ljust(15), '------'.ljust(15), '--------'.ljust(15), '----'))\n",
    "for t in text_meta:\n",
    "    print('{}{}{}{}'.format(t['title'].ljust(15), t['author'].ljust(15), t['language'].ljust(15), str(t['year']).ljust(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then insert the new texts with `TessMongoConnection.insert` after converting the raw JSON to Tesserae `Text` entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 4 texts.\n",
      "[ObjectId('5cd5d8a34273852ca631fafe'), ObjectId('5cd5d8a34273852ca631faff'), ObjectId('5cd5d8a34273852ca631fb00'), ObjectId('5cd5d8a34273852ca631fb01')]\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for t in text_meta:\n",
    "    texts.append(Text.json_decode(t))\n",
    "result = connection.insert(texts)\n",
    "print('Inserted {} texts.'.format(len(result.inserted_ids)))\n",
    "print(result.inserted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the inserted texts with `TessMongoConnection.find`. These texts will be converted to objects representing the database entries. The returned text list can be filtered by any valid field in the text database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title          Author         Language       Year\n",
      "aeneid         vergil         latin          19\n",
      "de oratore     cicero         latin          38\n",
      "heracles       euripides      greek          -416\n",
      "epistles       plato          greek          -280\n"
     ]
    }
   ],
   "source": [
    "texts = connection.find('texts', _id=result.inserted_ids)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "for t in texts:\n",
    "    print('{}{}{}{}'.format(t.title.ljust(15), t.author.ljust(15), t.language.ljust(15), t.year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading .tess Files\n",
    "\n",
    "Text metadata includes the path to the .tess file on the local filesystem. Using a Text retrieved from the database, the file can be loaded for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la/vergil.aeneid.tess\n",
      "9908\n",
      "<verg. aen. 1.271>\ttransferet, et longam multa vi muniet Albam.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tessfile = TessFile(texts[0].path, metadata=texts[0])\n",
    "\n",
    "print(tessfile.path)\n",
    "print(len(tessfile))\n",
    "print(tessfile[270])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the file line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<verg. aen. 1.1>\tArma virumque cano, Troiae qui primus ab oris\n",
      "\n",
      "<verg. aen. 1.2>\tItaliam, fato profugus, Laviniaque venit\n",
      "\n",
      "<verg. aen. 1.3>\tlitora, multum ille et terris iactatus et alto\n",
      "\n",
      "<verg. aen. 1.4>\tvi superum saevae memorem Iunonis ob iram;\n",
      "\n",
      "<verg. aen. 1.5>\tmulta quoque et bello passus, dum conderet urbem,\n",
      "\n",
      "<verg. aen. 1.6>\tinferretque deos Latio, genus unde Latinum,\n",
      "\n",
      "<verg. aen. 1.7>\tAlbanique patres, atque altae moenia Romae.\n",
      "\n",
      "<verg. aen. 1.8>\tMusa, mihi causas memora, quo numine laeso,\n",
      "\n",
      "<verg. aen. 1.9>\tquidve dolens, regina deum tot volvere casus\n",
      "\n",
      "<verg. aen. 1.10>\tinsignem pietate virum, tot adire labores\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = tessfile.readlines()\n",
    "for i in range(10):\n",
    "    print(next(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also iterate token-by-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arma\n",
      "virumque\n",
      "cano,\n",
      "Troiae\n",
      "qui\n",
      "primus\n",
      "ab\n",
      "oris\n",
      "Italiam,\n",
      "fato\n"
     ]
    }
   ],
   "source": [
    "tokens = tessfile.read_tokens()\n",
    "for i in range(10):\n",
    "    print(next(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing a Text\n",
    "\n",
    "Texts can be tokenized with `tesserae.tokenizers` objects. These objects are designed to normalize and compute features for tokens of a specific language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f41e23837c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGreekTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtessfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'greek'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mLatinTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtessfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtessfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tessv5/lib/python3.6/site-packages/tesserae/tokenizers/base.py\u001b[0m in \u001b[0;36mtokenize2\u001b[0;34m(self, raw, record, text)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mfeature_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                         \u001b[0mfeature_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0mnorm_i\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
    "\n",
    "tokens, tags, features = tokenizer.tokenize2(tessfile.read(), text=tessfile.metadata)\n",
    "\n",
    "print(len(tokens), len(tags), len(features))\n",
    "\n",
    "print('{}{}{}{}'.format('Raw'.ljust(15), 'Normalized'.ljust(15), 'Lemmata'.ljust(20), 'Frequency'))\n",
    "print('{}{}{}{}'.format('---'.ljust(15), '----------'.ljust(15), '-------'.ljust(20), '---------'))\n",
    "for i in range(20):\n",
    "    if len(tokens[i].features):\n",
    "        print('{}{}{}{}'.format(tokens[i].display.ljust(15),\n",
    "                              str(tokens[i].features['form'].token).ljust(20),\n",
    "                              str(tokens[i].features['lemmata'][0].token).ljust(20),\n",
    "                              list(tokens[i].features['form'].frequencies.values())[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed tokens can then be stored in and retrieved from the database, similar to text metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = connection.insert(features)\n",
    "print('Inserted {} feature entities out of {}'.format(len(result.inserted_ids), len(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitizing a Text\n",
    "\n",
    "Texts can be unitized into lines and phrases, and the intertext matches are found between units of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unitizing lines of a poem\n",
    "unitizer = Unitizer()\n",
    "lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
    "\n",
    "print('Lines\\n-----')\n",
    "for line in lines[:20]:\n",
    "        print(''.join([str(line.tags), ': '] + [t.display for t in line.tokens]))\n",
    "        \n",
    "print('\\n\\nPhrases\\n-------')\n",
    "for phrase in phrases[:20]:\n",
    "        print(''.join([str(phrase.tags), ': '] + [t.display for t in phrase.tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unitizing phrases of a poem or prose\n",
    "result = connection.insert(lines + phrases)\n",
    "print('Inserted {} units out of {}.'.format(len(result.inserted_ids), len(lines + phrases)))\n",
    "\n",
    "\n",
    "result = connection.insert(tokens)\n",
    "print('Inserted {} tokens out of {}.'.format(len(result.inserted_ids), len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for text in texts[1:]:\n",
    "    tessfile = TessFile(text.path, metadata=text)\n",
    "    tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
    "\n",
    "    \n",
    "    tokens, tags, frequencies, feature_sets = tokenizer.tokenize(tessfile.read(), text=tessfile.metadata)\n",
    "        \n",
    "    tokens = tokenizer.tokens\n",
    "    result = connection.insert(feature_sets)\n",
    "    result = connection.insert(frequencies)\n",
    "    \n",
    "    unitizer = Unitizer()\n",
    "    lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
    "    result = connection.insert(lines + phrases)\n",
    "    \n",
    "    result = connection.insert(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching\n",
    "\n",
    "Once the Texts, Tokens, and Units are in the database, we can then find intertext matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "matcher = AggregationMatcher(connection)\n",
    "match_texts = [t for t in texts if t.language == 'greek']\n",
    "\n",
    "start = time.time()\n",
    "matches, match_set = matcher.match(match_texts, 'phrase', 'form', distance_metric='span', stopwords=20, max_distance=10)\n",
    "print(\"Completed matching in {0:.2f}s\".format(time.time() - start))\n",
    "\n",
    "matches.sort(key=lambda x: x.score, reverse=True)\n",
    "\n",
    "# result = connection.insert(match_set)\n",
    "# print('Inserted {} match set entities out of {}'.format(len(result.inserted_ids), 1))\n",
    "result = connection.insert(matches)\n",
    "print('Inserted {} match entities out of {}'.format(len(result.inserted_ids), len(matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches = connection.aggregate('matches', [\n",
    "    {'$match': {'match_set': match_set.id}},\n",
    "    {'$sort': {'score': -1}},\n",
    "    {'$limit': 20},\n",
    "    {'$lookup': {\n",
    "        'from': 'units',\n",
    "        'let': {'m_units': '$units'},\n",
    "        'pipeline': [\n",
    "            {'$match': {'$expr': {'$in': ['$_id', '$$m_units']}}},\n",
    "            {'$lookup': {\n",
    "                'from': 'tokens',\n",
    "                'localField': '_id',\n",
    "                'foreignField': 'phrase',\n",
    "                'as': 'tokens'\n",
    "            }},\n",
    "            {'$sort': {'index': 1}}\n",
    "        ],\n",
    "        'as': 'units'\n",
    "    }},\n",
    "    {'$lookup': {\n",
    "        'from': 'tokens',\n",
    "        'localField': 'tokens',\n",
    "        'foreignField': '_id',\n",
    "        'as': 'tokens'\n",
    "    }},\n",
    "    {'$project': {\n",
    "        'units': True,\n",
    "        'score': True,\n",
    "        'tokens': '$tokens.feature_set'\n",
    "    }},\n",
    "    {'$lookup': {\n",
    "        'from': 'feature_sets',\n",
    "        'localField': 'tokens',\n",
    "        'foreignField': '_id',\n",
    "        'as': 'tokens'\n",
    "    }}\n",
    "])\n",
    "\n",
    "print('\\n')\n",
    "print('{}{}'.format('Score'.ljust(15), 'Match Tokens'.ljust(15)))\n",
    "print('{}{}'.format('-----'.ljust(15), '------------'.ljust(15)))\n",
    "for m in matches:\n",
    "    print('{}{}'.format(('%.3f'%(m.score)).ljust(15), ', '.join(list(set([t['form'] for t in m.tokens])))))\n",
    "    print('{} {} {}: {}'.format(match_texts[0].author, match_texts[0].title, m.units[0]['tags'], ''.join([t['display'] for t in m.units[0]['tokens']])))\n",
    "    print('{} {} {}: {}'.format(match_texts[1].author, match_texts[1].title, m.units[1]['tags'], ''.join([t['display'] for t in m.units[1]['tokens']])))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
