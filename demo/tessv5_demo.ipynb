{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesserae v5 Demo\n",
    "\n",
    "This demo will go over the basics of Tesserae v5 development up through October 11, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult at 0x7fb79b462648>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from tesserae.db import TessMongoConnection\n",
    "from tesserae.db.entities import Frequency, Match, Text, Token, Unit\n",
    "from tesserae.utils import TessFile\n",
    "from tesserae.tokenizers import GreekTokenizer, LatinTokenizer\n",
    "from tesserae.unitizer import Unitizer\n",
    "from tesserae.matchers import DefaultMatcher\n",
    "\n",
    "# Set up the connection and clean up the database\n",
    "connection = TessMongoConnection('45.55.219.221', 27017, None, None, 'tesstest')\n",
    "\n",
    "# Clean up the previous demo\n",
    "connection.connection['frequencies'].delete_many({})\n",
    "connection.connection['matches'].delete_many({})\n",
    "connection.connection['texts'].delete_many({})\n",
    "connection.connection['tokens'].delete_many({})\n",
    "connection.connection['units'].delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Storing New Texts\n",
    "\n",
    "The Tesserae database catalogs metadata, including the title, author, and year published, as well as integrity information like filepath, MD5 hash, and CTS URN.\n",
    "\n",
    "We start by loading in some metadata from `text_metadata.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title          Author         Language       Year\n",
      "-----          ------         --------       ----\n",
      "aeneid         vergil         latin          19             \n",
      "de oratore     cicero         latin          38             \n",
      "heracles       euripides      greek          -416           \n",
      "epistles       plato          greek          -280           \n"
     ]
    }
   ],
   "source": [
    "with open('text_metadata.json', 'r') as f:\n",
    "    text_meta = json.load(f)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "print('{}{}{}{}'.format('-----'.ljust(15), '------'.ljust(15), '--------'.ljust(15), '----'))\n",
    "for t in text_meta:\n",
    "    print('{}{}{}{}'.format(t['title'].ljust(15), t['author'].ljust(15), t['language'].ljust(15), str(t['year']).ljust(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then insert the new texts with `tesserae.text_access.insert_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'collection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-540df7e698ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_meta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minserted_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minserted_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/tesserae-v5/tess-env/lib/python3.6/site-packages/tesserae/db/mongodb.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, entity)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             exists = self.find(entity.collection,\n\u001b[0m\u001b[1;32m    121\u001b[0m                                **entity.to_json(exclude='_id'))\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'collection'"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for t in text_meta:\n",
    "    texts.append(Text.json_decode(t))\n",
    "result = connection.insert(texts)\n",
    "print(result.inserted_count)\n",
    "print(result.inserted_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the inserted texts with `tesserae.text_access.retrieve_text_list`. These texts will be converted to objects representing the database entries. The returned text list can be filtered by any valid field in the text database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = retrieve_text_list(client)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "for t in texts:\n",
    "    print('{}{}{}{}'.format(t.title.ljust(15), t.author.ljust(15), t.language.ljust(15), t.year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading .tess Files\n",
    "\n",
    "Text metadata includes the path to the .tess file on the local filesystem. Using a Text retrieved from the database, the file can be loaded for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessfile = load_text(client, texts[1].cts_urn)\n",
    "\n",
    "print(tessfile.path)\n",
    "print(len(tessfile))\n",
    "print(tessfile[270])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the file line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = tessfile.readlines()\n",
    "for i in range(10):\n",
    "    print(next(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also iterate token-by-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tessfile.read_tokens()\n",
    "for i in range(10):\n",
    "    print(next(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing a Text\n",
    "\n",
    "Texts can be tokenized with `tesserae.tokenizers.get_token_info`. This function takes a token and the language to use for lemmatization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "print('{}{}{}'.format('Raw'.ljust(15), 'Normalized'.ljust(15), 'Lemmata'))\n",
    "for i in range(10):\n",
    "    token = get_token_info(next(tokens), tessfile.metadata.language)\n",
    "    print('{}{}{}'.format(token.raw.ljust(15), token.token_type.ljust(15), token.lemmata))\n",
    "    tokenized.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed tokens can then be stored in and retrieved from the database, similar to text metadata. It should be noted that the resulting list is shorter than the original token list. During insertion, Tesserae removes duplicate tokens to prevent database bloat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client['tokens'].delete_many({})\n",
    "result = insert_tokens(client, tokenized)\n",
    "\n",
    "tokens = retrieve_token_list(client)\n",
    "for token in tokens:\n",
    "    print('{}{}{}'.format(token.raw.ljust(15), token.token_type.ljust(15), token.lemmata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitizing a Text\n",
    "\n",
    "Texts can be unitized into lines (poetry only) and phrases (poetry and prose), and the intertext matches are found between units of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitizing lines of a poem\n",
    "if tessfile.metadata.author in ['vergil', 'euripides']:\n",
    "    units = poetry.split_line_units(tessfile)\n",
    "    for i in range(10):\n",
    "        print(units[i].raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitizing phrases of a poem or prose\n",
    "if tessfile.metadata.author in ['vergil', 'euripides']:\n",
    "    units = poetry.split_phrase_units(tessfile)\n",
    "    for i in range(10):\n",
    "        print(units[i].raw)\n",
    "else:\n",
    "    units = prose.split_phrase_units(tessfile)\n",
    "    for i in range(10):\n",
    "        print(units[i].raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tess-env",
   "language": "python",
   "name": "tess-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
