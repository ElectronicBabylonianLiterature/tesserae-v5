{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesserae v5 Demo\n",
    "\n",
    "This demo will go over the basics of Tesserae v5 development up through October 11, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tesserae.db import TessMongoConnection\n",
    "from tesserae.db.entities import Frequency, Match, Text, Token, Unit\n",
    "from tesserae.utils import TessFile\n",
    "from tesserae.tokenizers import GreekTokenizer, LatinTokenizer\n",
    "from tesserae.unitizer import Unitizer\n",
    "from tesserae.matchers import DefaultMatcher\n",
    "\n",
    "# Set up the connection and clean up the database\n",
    "connection = TessMongoConnection('127.0.0.1', 27017, None, None, 'tesstest')\n",
    "\n",
    "# Clean up the previous demo\n",
    "connection.connection['frequencies'].delete_many({})\n",
    "connection.connection['matches'].delete_many({})\n",
    "connection.connection['texts'].delete_many({})\n",
    "connection.connection['tokens'].delete_many({})\n",
    "connection.connection['units'].delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Storing New Texts\n",
    "\n",
    "The Tesserae database catalogs metadata, including the title, author, and year published, as well as integrity information like filepath, MD5 hash, and CTS URN.\n",
    "\n",
    "We start by loading in some metadata from `text_metadata.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_metadata.json', 'r') as f:\n",
    "    text_meta = json.load(f)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "print('{}{}{}{}'.format('-----'.ljust(15), '------'.ljust(15), '--------'.ljust(15), '----'))\n",
    "for t in text_meta:\n",
    "    print('{}{}{}{}'.format(t['title'].ljust(15), t['author'].ljust(15), t['language'].ljust(15), str(t['year']).ljust(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then insert the new texts with `TessMongoConnection.insert` after converting the raw JSON to Tesserae `Text` entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for t in text_meta:\n",
    "    texts.append(Text.json_decode(t))\n",
    "result = connection.insert(texts)\n",
    "print('Inserted {} texts.'.format(len(result.inserted_ids)))\n",
    "print(result.inserted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the inserted texts with `TessMongoConnection.find`. These texts will be converted to objects representing the database entries. The returned text list can be filtered by any valid field in the text database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = connection.find('texts', _id=result.inserted_ids)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "for t in texts:\n",
    "    print('{}{}{}{}'.format(t.title.ljust(15), t.author.ljust(15), t.language.ljust(15), t.year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading .tess Files\n",
    "\n",
    "Text metadata includes the path to the .tess file on the local filesystem. Using a Text retrieved from the database, the file can be loaded for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessfile = TessFile(texts[0].path, metadata=texts[0])\n",
    "\n",
    "print(tessfile.path)\n",
    "print(len(tessfile))\n",
    "print(tessfile[270])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the file line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = tessfile.readlines()\n",
    "for i in range(10):\n",
    "    print(next(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also iterate token-by-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tessfile.read_tokens()\n",
    "for i in range(10):\n",
    "    print(next(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing a Text\n",
    "\n",
    "Texts can be tokenized with `tesserae.tokenizers.get_token_info`. This function takes a token and the language to use for lemmatization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GreekTokenizer() if tessfile.metadata.language == 'greek' else LatinTokenizer()\n",
    "\n",
    "for i, line in enumerate(tessfile.readlines(include_tag=False)):\n",
    "    if i > 9:\n",
    "        break\n",
    "    tokens, frequencies = tokenizer.tokenize(line, text=tessfile.metadata)\n",
    "\n",
    "tokens = tokenizer.tokens    \n",
    "\n",
    "print('{}{}{}'.format('Raw'.ljust(15), 'Normalized'.ljust(15), 'Lemmata'))\n",
    "print('{}{}{}'.format('---'.ljust(15), '----------'.ljust(15), '-------'))\n",
    "for i in range(10):\n",
    "    print('{}{}{}'.format(tokenizer.tokens[i].display.ljust(15),\n",
    "                          str(tokenizer.tokens[i].form).ljust(15),\n",
    "                          tokenizer.tokens[i].lemmata))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed tokens can then be stored in and retrieved from the database, similar to text metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = connection.insert(tokens)\n",
    "print('Inserted {} tokens out of {}.'.format(len(result.inserted_ids), len(tokens)))\n",
    "\n",
    "result = connection.insert(frequencies)\n",
    "print('Inserted {} frequency entities out of {}'.format(len(result.inserted_ids), len(frequencies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitizing a Text\n",
    "\n",
    "Texts can be unitized into lines and phrases, and the intertext matches are found between units of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitizing lines of a poem\n",
    "unitizer = Unitizer()\n",
    "lines, phrases = unitizer.unitize(tokens, tessfile.metadata)\n",
    "\n",
    "print('Lines\\n-----')\n",
    "for line in lines:\n",
    "        print(''.join([tokens[t].display for t in line.tokens]))\n",
    "        \n",
    "print('\\n\\nPhrases\\n-------')\n",
    "for phrase in phrases:\n",
    "        print(''.join([tokens[t].display for t in phrase.tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitizing phrases of a poem or prose\n",
    "result = connection.insert(lines + phrases)\n",
    "print('Inserted {} units out of {}.'.format(len(result.inserted_ids), len(lines + phrases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts[1:]:\n",
    "    tessfile = TessFile(text.path, metadata=text)\n",
    "    tokenizer = GreekTokenizer() if tessfile.metadata.language == 'greek' else LatinTokenizer()\n",
    "\n",
    "    for i, line in enumerate(tessfile.readlines(include_tag=False)):\n",
    "        if i > 9:\n",
    "            break\n",
    "        tokens, frequencies = tokenizer.tokenize(line, text=tessfile.metadata)\n",
    "        \n",
    "    tokens = tokenizer.tokens\n",
    "    result = connection.insert(tokens)\n",
    "    result = connection.insert(frequencies)\n",
    "    \n",
    "    unitizer = Unitizer()\n",
    "    lines, phrases = unitizer.unitize(tokens, tessfile.metadata)\n",
    "    result = connection.insert(lines + phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching\n",
    "\n",
    "Once the Texts, Tokens, and Units are in the database, we can then find intertext matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = DefaultMatcher(connection)\n",
    "match_texts = [t for t in texts if t.language == 'latin']\n",
    "matches = matcher.match(match_texts, 'phrase', 'lemmata', distance_metric='frequency', max_distance=999)\n",
    "matches.sort(key=lambda x: x.score, reverse=True)\n",
    "\n",
    "tokens = [connection.find('tokens', text=t.path) for t in match_texts]\n",
    "\n",
    "print('\\n')\n",
    "print('{}{}{}'.format('Score'.ljust(15), 'Match Tokens'.ljust(15), 'Source Text'.ljust(15), 'Target Text'))\n",
    "print('{}{}{}'.format('-----'.ljust(15), '------------'.ljust(15), '-----------'.ljust(15), '-----------'))\n",
    "for m in matches:\n",
    "    print('{}{}'.format(('%.3f'%(m.score)).ljust(15), ', '.join(list(set([t.form for t in m.match_tokens[0]])))))\n",
    "    print('Source Text: {}'.format(''.join([tokens[0][t].display for t in m.units[0].tokens])))\n",
    "    print('Target Text: {}'.format(''.join([tokens[1][t].display for t in m.units[1].tokens])))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tess-env",
   "language": "python",
   "name": "tess-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
