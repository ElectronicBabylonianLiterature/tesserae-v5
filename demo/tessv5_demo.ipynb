{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesserae v5 Demo\n",
    "\n",
    "This demo will go over the basics of Tesserae v5 development up through October 11, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult at 0x7f2fa7606848>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from tesserae.db import TessMongoConnection\n",
    "from tesserae.db.entities import Frequency, Match, Text, Token, Unit\n",
    "from tesserae.utils import TessFile\n",
    "from tesserae.tokenizers import GreekTokenizer, LatinTokenizer\n",
    "from tesserae.unitizer import Unitizer\n",
    "from tesserae.matchers import AggregationMatcher\n",
    "\n",
    "# Set up the connection and clean up the database\n",
    "connection = TessMongoConnection('127.0.0.1', 27017, None, None, 'tesstest')\n",
    "\n",
    "# Clean up the previous demo\n",
    "connection.connection['feature_sets'].delete_many({})\n",
    "connection.connection['frequencies'].delete_many({})\n",
    "connection.connection['matches'].delete_many({})\n",
    "connection.connection['match_sets'].delete_many({})\n",
    "connection.connection['texts'].delete_many({})\n",
    "connection.connection['tokens'].delete_many({})\n",
    "connection.connection['units'].delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Storing New Texts\n",
    "\n",
    "The Tesserae database catalogs metadata, including the title, author, and year published, as well as integrity information like filepath, MD5 hash, and CTS URN.\n",
    "\n",
    "We start by loading in some metadata from `text_metadata.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title          Author         Language       Year\n",
      "-----          ------         --------       ----\n",
      "aeneid         vergil         latin          19             \n",
      "de oratore     cicero         latin          38             \n",
      "heracles       euripides      greek          -416           \n",
      "epistles       plato          greek          -280           \n"
     ]
    }
   ],
   "source": [
    "with open('text_metadata.json', 'r') as f:\n",
    "    text_meta = json.load(f)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "print('{}{}{}{}'.format('-----'.ljust(15), '------'.ljust(15), '--------'.ljust(15), '----'))\n",
    "for t in text_meta:\n",
    "    print('{}{}{}{}'.format(t['title'].ljust(15), t['author'].ljust(15), t['language'].ljust(15), str(t['year']).ljust(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then insert the new texts with `TessMongoConnection.insert` after converting the raw JSON to Tesserae `Text` entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 4 texts.\n",
      "[ObjectId('5c47b6aadb8a82060de40cda'), ObjectId('5c47b6aadb8a82060de40cdb'), ObjectId('5c47b6aadb8a82060de40cdc'), ObjectId('5c47b6aadb8a82060de40cdd')]\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for t in text_meta:\n",
    "    texts.append(Text.json_decode(t))\n",
    "result = connection.insert(texts)\n",
    "print('Inserted {} texts.'.format(len(result.inserted_ids)))\n",
    "print(result.inserted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the inserted texts with `TessMongoConnection.find`. These texts will be converted to objects representing the database entries. The returned text list can be filtered by any valid field in the text database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title          Author         Language       Year\n",
      "aeneid         vergil         latin          19\n",
      "de oratore     cicero         latin          38\n",
      "heracles       euripides      greek          -416\n",
      "epistles       plato          greek          -280\n"
     ]
    }
   ],
   "source": [
    "texts = connection.find('texts', _id=result.inserted_ids)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "for t in texts:\n",
    "    print('{}{}{}{}'.format(t.title.ljust(15), t.author.ljust(15), t.language.ljust(15), t.year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading .tess Files\n",
    "\n",
    "Text metadata includes the path to the .tess file on the local filesystem. Using a Text retrieved from the database, the file can be loaded for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la/vergil.aeneid.tess\n",
      "9908\n",
      "<verg. aen. 1.271>\ttransferet, et longam multa vi muniet Albam.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tessfile = TessFile(texts[0].path, metadata=texts[0])\n",
    "\n",
    "print(tessfile.path)\n",
    "print(len(tessfile))\n",
    "print(tessfile[270])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the file line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<verg. aen. 1.1>\tArma virumque cano, Troiae qui primus ab oris\n",
      "\n",
      "<verg. aen. 1.2>\tItaliam, fato profugus, Laviniaque venit\n",
      "\n",
      "<verg. aen. 1.3>\tlitora, multum ille et terris iactatus et alto\n",
      "\n",
      "<verg. aen. 1.4>\tvi superum saevae memorem Iunonis ob iram;\n",
      "\n",
      "<verg. aen. 1.5>\tmulta quoque et bello passus, dum conderet urbem,\n",
      "\n",
      "<verg. aen. 1.6>\tinferretque deos Latio, genus unde Latinum,\n",
      "\n",
      "<verg. aen. 1.7>\tAlbanique patres, atque altae moenia Romae.\n",
      "\n",
      "<verg. aen. 1.8>\tMusa, mihi causas memora, quo numine laeso,\n",
      "\n",
      "<verg. aen. 1.9>\tquidve dolens, regina deum tot volvere casus\n",
      "\n",
      "<verg. aen. 1.10>\tinsignem pietate virum, tot adire labores\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = tessfile.readlines()\n",
    "for i in range(10):\n",
    "    print(next(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also iterate token-by-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arma\n",
      "virumque\n",
      "cano,\n",
      "Troiae\n",
      "qui\n",
      "primus\n",
      "ab\n",
      "oris\n",
      "Italiam,\n",
      "fato\n"
     ]
    }
   ],
   "source": [
    "tokens = tessfile.read_tokens()\n",
    "for i in range(10):\n",
    "    print(next(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing a Text\n",
    "\n",
    "Texts can be tokenized with `tesserae.tokenizers.get_token_info`. This function takes a token and the language to use for lemmatization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140667 16602 16602\n",
      "Raw            Normalized     Lemmata        Frequency\n",
      "---            ----------     -------        ---------\n",
      "Arma           arma           ['arma', 'armo']148\n",
      "virumque       uirumque       ['uir', 'uirus']4\n",
      "cano           cano           ['canus', 'cano']3\n",
      "Troiae         troiae         ['troius', 'troia']36\n",
      "qui            qui            ['quis', 'qui']166\n",
      "primus         primus         ['primus']     34\n",
      "ab             ab             ['ab']         136\n",
      "oris           oris           ['ora', 'os', 'aurum']32\n",
      "Italiam        italiam        ['italia']     32\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
    "\n",
    "tokens, frequencies, feature_sets = tokenizer.tokenize(tessfile.read(), text=tessfile.metadata)\n",
    "\n",
    "tokens = tokenizer.tokens\n",
    "print(len(tokens), len(frequencies), len(feature_sets))\n",
    "\n",
    "print('{}{}{}{}'.format('Raw'.ljust(15), 'Normalized'.ljust(15), 'Lemmata'.ljust(15), 'Frequency'))\n",
    "print('{}{}{}{}'.format('---'.ljust(15), '----------'.ljust(15), '-------'.ljust(15), '---------'))\n",
    "for i in range(20):\n",
    "    if tokenizer.tokens[i].feature_set is not None:\n",
    "        print('{}{}{}{}'.format(tokenizer.tokens[i].display.ljust(15),\n",
    "                              str(tokenizer.tokens[i].feature_set.form).ljust(15),\n",
    "                              str(tokenizer.tokens[i].feature_set.lemmata).ljust(15),\n",
    "                              tokenizer.tokens[i].frequency.frequency))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed tokens can then be stored in and retrieved from the database, similar to text metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 16602 feature set entities out of 16602\n",
      "Inserted 16602 frequency entities out of 16602\n",
      "Inserted 140667 tokens out of 140667.\n"
     ]
    }
   ],
   "source": [
    "result = connection.insert(feature_sets)\n",
    "print('Inserted {} feature set entities out of {}'.format(len(result.inserted_ids), len(feature_sets)))\n",
    "\n",
    "result = connection.insert(frequencies)\n",
    "print('Inserted {} frequency entities out of {}'.format(len(result.inserted_ids), len(frequencies)))\n",
    "\n",
    "result = connection.insert(tokens)\n",
    "print('Inserted {} tokens out of {}.'.format(len(result.inserted_ids), len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitizing a Text\n",
    "\n",
    "Texts can be unitized into lines and phrases, and the intertext matches are found between units of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines\n",
      "-----\n",
      "Arma virumque cano, Troiae qui primus ab oris / \n",
      "Italiam, fato profugus, Laviniaque venit / \n",
      "litora, multum ille et terris iactatus et alto / \n",
      "vi superum saevae memorem Iunonis ob iram; / \n",
      "multa quoque et bello passus, dum conderet urbem, / \n",
      "inferretque deos Latio, genus unde Latinum, / \n",
      "Albanique patres, atque altae moenia Romae. / \n",
      "Musa, mihi causas memora, quo numine laeso, / \n",
      "quidve dolens, regina deum tot volvere casus / \n",
      "insignem pietate virum, tot adire labores / \n",
      "impulerit. Tantaene animis caelestibus irae? / \n",
      "Urbs antiqua fuit, Tyrii tenuere coloni, / \n",
      "Karthago, Italiam contra Tiberinaque longe / \n",
      "ostia, dives opum studiisque asperrima belli; / \n",
      "quam Iuno fertur terris magis omnibus unam / \n",
      "posthabita coluisse Samo; hic illius arma, / \n",
      "hic currus fuit; hoc regnum dea gentibus esse, / \n",
      "si qua fata sinant, iam tum tenditque fovetque. / \n",
      "Progeniem sed enim Troiano a sanguine duci / \n",
      "audierat, Tyrias olim quae verteret arces; / \n",
      "\n",
      "\n",
      "Phrases\n",
      "-------\n",
      "Arma virumque cano, Troiae qui primus ab oris / Italiam, fato profugus, Laviniaque venit / litora, multum ille et terris iactatus et alto / vi superum saevae memorem Iunonis ob iram; / \n",
      "multa quoque et bello passus, dum conderet urbem, / inferretque deos Latio, genus unde Latinum, / Albanique patres, atque altae moenia Romae. / \n",
      "Musa, mihi causas memora, quo numine laeso, / quidve dolens, regina deum tot volvere casus / insignem pietate virum, tot adire labores / impulerit. \n",
      "Tantaene animis caelestibus irae? / \n",
      "Urbs antiqua fuit, Tyrii tenuere coloni, / Karthago, Italiam contra Tiberinaque longe / ostia, dives opum studiisque asperrima belli; / \n",
      "quam Iuno fertur terris magis omnibus unam / posthabita coluisse Samo; \n",
      "hic illius arma, / hic currus fuit; \n",
      "hoc regnum dea gentibus esse, / si qua fata sinant, iam tum tenditque fovetque. / \n",
      "Progeniem sed enim Troiano a sanguine duci / audierat, Tyrias olim quae verteret arces; / \n",
      "hinc populum late regem belloque superbum / venturum excidio Libyae: \n",
      "sic volvere Parcas. / \n",
      "Id metuens, veterisque memor Saturnia belli, / prima quod ad Troiam pro caris gesserat Argis--- / necdum etiam causae irarum saevique dolores / exciderant animo: \n",
      "manet alta mente repostum / iudicium Paridis spretaeque iniuria formae, / et genus invisum, et rapti Ganymedis honores. / \n",
      "His accensa super, iactatos aequore toto / Troas, reliquias Danaum atque immitis Achilli, / arcebat longe Latio, multosque per annos / errabant, acti fatis, maria omnia circum. / \n",
      "Tantae molis erat Romanam condere gentem! / \n",
      "Vix e conspectu Siculae telluris in altum / vela dabant laeti, et spumas salis aere ruebant, / cum Iuno, aeternum servans sub pectore volnus, / haec secum: \"\n",
      "Mene incepto desistere victam, / nec posse Italia Teucrorum avertere regem? / \n",
      "Quippe vetor fatis. \n",
      "Pallasne exurere classem / Argivum atque ipsos potuit submergere ponto, / unius ob noxam et furias Aiacis Oilei? / \n",
      "Ipsa, Iovis rapidum iaculata e nubibus ignem, / disiecitque rates evertitque aequora ventis, / illum expirantem transfixo pectore flammas / turbine corripuit scopuloque infixit acuto. / \n"
     ]
    }
   ],
   "source": [
    "# Unitizing lines of a poem\n",
    "unitizer = Unitizer()\n",
    "lines, phrases = unitizer.unitize(tokens, tessfile.metadata)\n",
    "\n",
    "print('Lines\\n-----')\n",
    "for line in lines[:20]:\n",
    "        print(''.join([t.display for t in line.tokens]))\n",
    "        \n",
    "print('\\n\\nPhrases\\n-------')\n",
    "for phrase in phrases[:20]:\n",
    "        print(''.join([t.display for t in phrase.tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 14997 units out of 14997.\n"
     ]
    }
   ],
   "source": [
    "# Unitizing phrases of a poem or prose\n",
    "result = connection.insert(lines + phrases)\n",
    "print('Inserted {} units out of {}.'.format(len(result.inserted_ids), len(lines + phrases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for text in texts[1:]:\n",
    "    tessfile = TessFile(text.path, metadata=text)\n",
    "    tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
    "\n",
    "    \n",
    "    tokens, frequencies, feature_sets = tokenizer.tokenize(tessfile.read(), text=tessfile.metadata)\n",
    "        \n",
    "    tokens = tokenizer.tokens\n",
    "    result = connection.insert(feature_sets)\n",
    "    result = connection.insert(frequencies)\n",
    "    result = connection.insert(tokens)\n",
    "    \n",
    "    unitizer = Unitizer()\n",
    "    lines, phrases = unitizer.unitize(tokens, tessfile.metadata)\n",
    "    result = connection.insert(lines + phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching\n",
    "\n",
    "Once the Texts, Tokens, and Units are in the database, we can then find intertext matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['et', 'quis', 'in', 'hic', 'dico', 'sum', 'tu', 'ego', 'neque', 'atque']\n"
     ]
    },
    {
     "ename": "OperationFailure",
     "evalue": "BSONObj size: 35120449 (0x217E541) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: { 5c47b6aadb8a82060de40cda: null, 5c47b6aadb8a82060de40cdb: null }",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationFailure\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6a7d4a54dead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAggregationMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmatch_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'latin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'phrase'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lemmata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'frequency'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tessv5/lib/python3.6/site-packages/tesserae/matchers/aggregator.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(self, texts, unit_type, feature, stopwords, stopword_basis, score_basis, frequency_basis, max_distance, distance_metric)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;31m#                  remove_non_matches])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0magg_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_matches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tessv5/lib/python3.6/site-packages/tesserae/db/mongodb.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, collection, pipeline, encode)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \"\"\"\n\u001b[1;32m     89\u001b[0m         result = self.connection[collection].aggregate(pipeline,\n\u001b[0;32m---> 90\u001b[0;31m                                                        allowDiskUse=True)\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tessv5/lib/python3.6/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, pipeline, session, **kwargs)\u001b[0m\n\u001b[1;32m   2395\u001b[0m                                    \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m                                    \u001b[0mexplicit_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m                                    **kwargs)\n\u001b[0m\u001b[1;32m   2398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maggregate_raw_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tessv5/lib/python3.6/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36m_aggregate\u001b[0;34m(self, pipeline, cursor_class, first_batch_size, session, explicit_session, **kwargs)\u001b[0m\n\u001b[1;32m   2302\u001b[0m                 \u001b[0mcollation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m                 \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2304\u001b[0;31m                 client=self.__database.client)\n\u001b[0m\u001b[1;32m   2305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"cursor\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tessv5/lib/python3.6/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events)\u001b[0m\n\u001b[1;32m    577\u001b[0m                            \u001b[0mcompression_ctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                            \u001b[0muse_op_msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_msg_enabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m                            unacknowledged=unacknowledged)\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOperationFailure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tessv5/lib/python3.6/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(sock, dbname, spec, slave_ok, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, check_keys, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 helpers._check_command_response(\n\u001b[1;32m    149\u001b[0m                     \u001b[0mresponse_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowable_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                     parse_write_concern_error=parse_write_concern_error)\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpublish\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tessv5/lib/python3.6/site-packages/pymongo/helpers.py\u001b[0m in \u001b[0;36m_check_command_response\u001b[0;34m(response, msg, allowable_errors, parse_write_concern_error)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"%s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOperationFailure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merrmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationFailure\u001b[0m: BSONObj size: 35120449 (0x217E541) is invalid. Size must be between 0 and 16793600(16MB) First element: _id: { 5c47b6aadb8a82060de40cda: null, 5c47b6aadb8a82060de40cdb: null }"
     ]
    }
   ],
   "source": [
    "matcher = AggregationMatcher(connection)\n",
    "match_texts = [t for t in texts if t.language == 'latin']\n",
    "matches = matcher.match(match_texts, 'phrase', 'lemmata', distance_metric='frequency', max_distance=999)\n",
    "matches.sort(key=lambda x: x.score, reverse=True)\n",
    "\n",
    "tokens = [connection.find('tokens', text=t.path) for t in match_texts]\n",
    "\n",
    "print('\\n')\n",
    "print('{}{}{}'.format('Score'.ljust(15), 'Match Tokens'.ljust(15), 'Source Text'.ljust(15), 'Target Text'))\n",
    "print('{}{}{}'.format('-----'.ljust(15), '------------'.ljust(15), '-----------'.ljust(15), '-----------'))\n",
    "for m in matches:\n",
    "    print('{}{}'.format(('%.3f'%(m.score)).ljust(15), ', '.join(list(set([t.form for t in m.match_tokens[0]])))))\n",
    "    print('Source Text: {}'.format(''.join([tokens[0][t].display for t in m.units[0].tokens])))\n",
    "    print('Target Text: {}'.format(''.join([tokens[1][t].display for t in m.units[1].tokens])))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tessv5]",
   "language": "python",
   "name": "conda-env-tessv5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
