{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "tessv5_demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElectronicBabylonianLiterature/tesserae-v5/blob/master/demo/tessv5_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPoJ48OmfN-Z",
        "colab_type": "text"
      },
      "source": [
        "# Tesserae v5 Demo\n",
        "\n",
        "This demo will go over the basics of Tesserae v5 development up through February 5, 2019."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTHNox59fPef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "18e77ab6-1081-4755-b4e4-db984d15733c"
      },
      "source": [
        "!pip install git+https://github.com/ElectronicBabylonianLiterature/tesserae-v5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\n",
            "Collecting git+https://github.com/ElectronicBabylonianLiterature/tesserae-v5\n",
            "  Cloning https://github.com/ElectronicBabylonianLiterature/tesserae-v5 to /tmp/pip-req-build-bvxv10dl\n",
            "  Running command git clone -q https://github.com/ElectronicBabylonianLiterature/tesserae-v5 /tmp/pip-req-build-bvxv10dl\n",
            "Collecting cltk>=0.1.83\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/20/42060504debda1fe85808d32ae82344e2efa4343dda15ec0151cf6bfe13d/cltk-0.1.121.tar.gz (625kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.5 in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (3.2.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (1.18.5)\n",
            "Requirement already satisfied: pymongo>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (3.10.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (4.41.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (5.5.0)\n",
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/f9/c315aa88e51fabdc08e91b333cfefb255aff04a2ee96d632c32cb19180c9/GitPython-3.1.3-py3-none-any.whl (451kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 29.6MB/s \n",
            "\u001b[?25hCollecting python-crfsuite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 15.5MB/s \n",
            "\u001b[?25hCollecting pyuca\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/88/aeeee34d88f841aca712a8c18fbd62a33eaad8f2dbe535e87f3c829b02f9/pyuca-1.2-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 34.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from cltk>=0.1.83->tesserae==0.1a1) (3.13)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from cltk>=0.1.83->tesserae==0.1a1) (2019.12.20)\n",
            "Collecting whoosh\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/19/24d0f1f454a2c1eb689ca28d2f178db81e5024f42d82729a4ff6771155cf/Whoosh-2.7.4-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.5->tesserae==0.1a1) (1.12.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.8MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: tesserae, cltk\n",
            "  Building wheel for tesserae (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tesserae: filename=tesserae-0.1a1-cp36-none-any.whl size=61754 sha256=0d15788f058c9dc10251dd40f3d983114b8481ae692eac07fd14b7e7396a40fb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qlikw8c6/wheels/15/12/9b/1a9461e40ceddee12498dee0f289daa405301c5f616adf3a1c\n",
            "  Building wheel for cltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cltk: filename=cltk-0.1.121-cp36-none-any.whl size=711645 sha256=d5f71e1966d7437ada84b35b4f081c986339e897ee9d8a9a076874be6e098eb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/c9/6b/e60acb6f511ebe008f3e961e894d57598517b25c4cbffbb70f\n",
            "Successfully built tesserae cltk\n",
            "Installing collected packages: smmap, gitdb, gitpython, python-crfsuite, pyuca, whoosh, cltk, tesserae\n",
            "Successfully installed cltk-0.1.121 gitdb-4.0.5 gitpython-3.1.3 python-crfsuite-0.9.7 pyuca-1.2 smmap-3.0.4 tesserae-0.1a1 whoosh-2.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIR-RVZbfN-a",
        "colab_type": "code",
        "colab": {},
        "outputId": "c1e707bb-6e5f-4c4a-f96f-5860134529ac"
      },
      "source": [
        "import json\n",
        "\n",
        "from tesserae.db import TessMongoConnection\n",
        "from tesserae.db.entities import Frequency, Match, Text, Token, Unit\n",
        "from tesserae.utils import TessFile\n",
        "from tesserae.tokenizers import GreekTokenizer, LatinTokenizer\n",
        "from tesserae.unitizer import Unitizer\n",
        "from tesserae.matchers import AggregationMatcher\n",
        "from tesserae.matchers.sparse_encoding import SparseMatrixSearch\n",
        "\n",
        "# Set up the connection and clean up the database\n",
        "connection = TessMongoConnection('127.0.0.1', 27017, None, None, 'tesstest')\n",
        "\n",
        "# Clean up the previous demo\n",
        "connection.connection['feature_sets'].delete_many({})\n",
        "connection.connection['feature']\n",
        "connection.connection['frequencies'].delete_many({})\n",
        "connection.connection['matches'].delete_many({})\n",
        "connection.connection['match_sets'].delete_many({})\n",
        "connection.connection['texts'].delete_many({})\n",
        "connection.connection['tokens'].delete_many({})\n",
        "connection.connection['units'].delete_many({})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pymongo.results.DeleteResult at 0x7f9a96fb9f88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noYHkyCrfN-f",
        "colab_type": "text"
      },
      "source": [
        "## Loading and Storing New Texts\n",
        "\n",
        "The Tesserae database catalogs metadata, including the title, author, and year published, as well as integrity information like filepath, MD5 hash, and CTS URN.\n",
        "\n",
        "We start by loading in some metadata from `text_metadata.json`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNGacqEVfN-g",
        "colab_type": "code",
        "colab": {},
        "outputId": "60610d94-6a65-4ead-b205-c04d7b7141de"
      },
      "source": [
        "with open('text_metadata.json', 'r') as f:\n",
        "    text_meta = json.load(f)\n",
        "\n",
        "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
        "print('{}{}{}{}'.format('-----'.ljust(15), '------'.ljust(15), '--------'.ljust(15), '----'))\n",
        "for t in text_meta:\n",
        "    print('{}{}{}{}'.format(t['title'].ljust(15), t['author'].ljust(15), t['language'].ljust(15), str(t['year']).ljust(15)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title          Author         Language       Year\n",
            "-----          ------         --------       ----\n",
            "aeneid         vergil         latin          19             \n",
            "de oratore     cicero         latin          38             \n",
            "heracles       euripides      greek          -416           \n",
            "epistles       plato          greek          -280           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGn9rNDmfN-j",
        "colab_type": "text"
      },
      "source": [
        "Then insert the new texts with `TessMongoConnection.insert` after converting the raw JSON to Tesserae `Text` entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEjoXuPcfN-j",
        "colab_type": "code",
        "colab": {},
        "outputId": "e06d243e-e0d1-407a-a4d0-acfe348bf899"
      },
      "source": [
        "texts = []\n",
        "for t in text_meta:\n",
        "    texts.append(Text.json_decode(t))\n",
        "result = connection.insert(texts)\n",
        "print('Inserted {} texts.'.format(len(result.inserted_ids)))\n",
        "print(result.inserted_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inserted 4 texts.\n",
            "[ObjectId('5cd5d8a34273852ca631fafe'), ObjectId('5cd5d8a34273852ca631faff'), ObjectId('5cd5d8a34273852ca631fb00'), ObjectId('5cd5d8a34273852ca631fb01')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MUtvQfXfN-p",
        "colab_type": "text"
      },
      "source": [
        "We can retrieve the inserted texts with `TessMongoConnection.find`. These texts will be converted to objects representing the database entries. The returned text list can be filtered by any valid field in the text database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6S1JGudfN-q",
        "colab_type": "code",
        "colab": {},
        "outputId": "bc4bbabf-5ef6-4c53-891f-2fd895a7f5cf"
      },
      "source": [
        "texts = connection.find('texts', _id=result.inserted_ids)\n",
        "\n",
        "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
        "for t in texts:\n",
        "    print('{}{}{}{}'.format(t.title.ljust(15), t.author.ljust(15), t.language.ljust(15), t.year))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title          Author         Language       Year\n",
            "aeneid         vergil         latin          19\n",
            "de oratore     cicero         latin          38\n",
            "heracles       euripides      greek          -416\n",
            "epistles       plato          greek          -280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U4h9b7RfN-s",
        "colab_type": "text"
      },
      "source": [
        "## Loading .tess Files\n",
        "\n",
        "Text metadata includes the path to the .tess file on the local filesystem. Using a Text retrieved from the database, the file can be loaded for further processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jc_ruUyfN-s",
        "colab_type": "code",
        "colab": {},
        "outputId": "4419042e-3098-4b85-c540-fc5a5fb90585"
      },
      "source": [
        "tessfile = TessFile(texts[0].path, metadata=texts[0])\n",
        "\n",
        "print(tessfile.path)\n",
        "print(len(tessfile))\n",
        "print(tessfile[270])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "la/vergil.aeneid.tess\n",
            "9908\n",
            "<verg. aen. 1.271>\ttransferet, et longam multa vi muniet Albam.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyU-fhZmfN-v",
        "colab_type": "text"
      },
      "source": [
        "We can iterate through the file line-by-line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT5jDyUTfN-v",
        "colab_type": "code",
        "colab": {},
        "outputId": "c7bd8c07-8d50-4d93-d52d-73927ac09f3b"
      },
      "source": [
        "lines = tessfile.readlines()\n",
        "for i in range(10):\n",
        "    print(next(lines))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<verg. aen. 1.1>\tArma virumque cano, Troiae qui primus ab oris\n",
            "\n",
            "<verg. aen. 1.2>\tItaliam, fato profugus, Laviniaque venit\n",
            "\n",
            "<verg. aen. 1.3>\tlitora, multum ille et terris iactatus et alto\n",
            "\n",
            "<verg. aen. 1.4>\tvi superum saevae memorem Iunonis ob iram;\n",
            "\n",
            "<verg. aen. 1.5>\tmulta quoque et bello passus, dum conderet urbem,\n",
            "\n",
            "<verg. aen. 1.6>\tinferretque deos Latio, genus unde Latinum,\n",
            "\n",
            "<verg. aen. 1.7>\tAlbanique patres, atque altae moenia Romae.\n",
            "\n",
            "<verg. aen. 1.8>\tMusa, mihi causas memora, quo numine laeso,\n",
            "\n",
            "<verg. aen. 1.9>\tquidve dolens, regina deum tot volvere casus\n",
            "\n",
            "<verg. aen. 1.10>\tinsignem pietate virum, tot adire labores\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehzPA8bfN-y",
        "colab_type": "text"
      },
      "source": [
        "We can also iterate token-by-token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMlabOnkfN-y",
        "colab_type": "code",
        "colab": {},
        "outputId": "e9e15758-f4dc-405a-b90c-8ac8f028a5a5"
      },
      "source": [
        "tokens = tessfile.read_tokens()\n",
        "for i in range(10):\n",
        "    print(next(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arma\n",
            "virumque\n",
            "cano,\n",
            "Troiae\n",
            "qui\n",
            "primus\n",
            "ab\n",
            "oris\n",
            "Italiam,\n",
            "fato\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3skP5flfN-1",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing a Text\n",
        "\n",
        "Texts can be tokenized with `tesserae.tokenizers` objects. These objects are designed to normalize and compute features for tokens of a specific language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08wu-WZrfN-1",
        "colab_type": "code",
        "colab": {},
        "outputId": "f823e1b5-cbb4-4b60-aa92-4e984b14279c"
      },
      "source": [
        "tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
        "\n",
        "tokens, tags, features = tokenizer.tokenize2(tessfile.read(), text=tessfile.metadata)\n",
        "\n",
        "print(len(tokens), len(tags), len(features))\n",
        "\n",
        "print('{}{}{}{}'.format('Raw'.ljust(15), 'Normalized'.ljust(15), 'Lemmata'.ljust(20), 'Frequency'))\n",
        "print('{}{}{}{}'.format('---'.ljust(15), '----------'.ljust(15), '-------'.ljust(20), '---------'))\n",
        "for i in range(20):\n",
        "    if len(tokens[i].features):\n",
        "        print('{}{}{}{}'.format(tokens[i].display.ljust(15),\n",
        "                              str(tokens[i].features['form'].token).ljust(20),\n",
        "                              str(tokens[i].features['lemmata'][0].token).ljust(20),\n",
        "                              list(tokens[i].features['form'].frequencies.values())[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MemoryError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f41e23837c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGreekTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtessfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'greek'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mLatinTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtessfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtessfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tessv5/lib/python3.6/site-packages/tesserae/tokenizers/base.py\u001b[0m in \u001b[0;36mtokenize2\u001b[0;34m(self, raw, record, text)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mfeature_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                         \u001b[0mfeature_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0mnorm_i\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMemoryError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0k_igU5fN-4",
        "colab_type": "text"
      },
      "source": [
        "Processed tokens can then be stored in and retrieved from the database, similar to text metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urZTBdFzfN-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = connection.insert(features)\n",
        "print('Inserted {} feature entities out of {}'.format(len(result.inserted_ids), len(features)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuLJFTw2fN-6",
        "colab_type": "text"
      },
      "source": [
        "## Unitizing a Text\n",
        "\n",
        "Texts can be unitized into lines and phrases, and the intertext matches are found between units of text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oOUmwI7fN-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unitizing lines of a poem\n",
        "unitizer = Unitizer()\n",
        "lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
        "\n",
        "print('Lines\\n-----')\n",
        "for line in lines[:20]:\n",
        "        print(''.join([str(line.tags), ': '] + [t.display for t in line.tokens]))\n",
        "        \n",
        "print('\\n\\nPhrases\\n-------')\n",
        "for phrase in phrases[:20]:\n",
        "        print(''.join([str(phrase.tags), ': '] + [t.display for t in phrase.tokens]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwKa94FEfN-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unitizing phrases of a poem or prose\n",
        "result = connection.insert(lines + phrases)\n",
        "print('Inserted {} units out of {}.'.format(len(result.inserted_ids), len(lines + phrases)))\n",
        "\n",
        "\n",
        "result = connection.insert(tokens)\n",
        "print('Inserted {} tokens out of {}.'.format(len(result.inserted_ids), len(tokens)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOdy-Up5fN-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for text in texts[1:]:\n",
        "    tessfile = TessFile(text.path, metadata=text)\n",
        "    tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
        "\n",
        "    \n",
        "    tokens, tags, frequencies, feature_sets = tokenizer.tokenize(tessfile.read(), text=tessfile.metadata)\n",
        "        \n",
        "    tokens = tokenizer.tokens\n",
        "    result = connection.insert(feature_sets)\n",
        "    result = connection.insert(frequencies)\n",
        "    \n",
        "    unitizer = Unitizer()\n",
        "    lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
        "    result = connection.insert(lines + phrases)\n",
        "    \n",
        "    result = connection.insert(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi09bzDPfN_C",
        "colab_type": "text"
      },
      "source": [
        "## Matching\n",
        "\n",
        "Once the Texts, Tokens, and Units are in the database, we can then find intertext matches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEJyHxDCfN_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "matcher = AggregationMatcher(connection)\n",
        "match_texts = [t for t in texts if t.language == 'greek']\n",
        "\n",
        "start = time.time()\n",
        "matches, match_set = matcher.match(match_texts, 'phrase', 'form', distance_metric='span', stopwords=20, max_distance=10)\n",
        "print(\"Completed matching in {0:.2f}s\".format(time.time() - start))\n",
        "\n",
        "matches.sort(key=lambda x: x.score, reverse=True)\n",
        "\n",
        "# result = connection.insert(match_set)\n",
        "# print('Inserted {} match set entities out of {}'.format(len(result.inserted_ids), 1))\n",
        "result = connection.insert(matches)\n",
        "print('Inserted {} match entities out of {}'.format(len(result.inserted_ids), len(matches)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzLx0OqgfN_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matches = connection.aggregate('matches', [\n",
        "    {'$match': {'match_set': match_set.id}},\n",
        "    {'$sort': {'score': -1}},\n",
        "    {'$limit': 20},\n",
        "    {'$lookup': {\n",
        "        'from': 'units',\n",
        "        'let': {'m_units': '$units'},\n",
        "        'pipeline': [\n",
        "            {'$match': {'$expr': {'$in': ['$_id', '$$m_units']}}},\n",
        "            {'$lookup': {\n",
        "                'from': 'tokens',\n",
        "                'localField': '_id',\n",
        "                'foreignField': 'phrase',\n",
        "                'as': 'tokens'\n",
        "            }},\n",
        "            {'$sort': {'index': 1}}\n",
        "        ],\n",
        "        'as': 'units'\n",
        "    }},\n",
        "    {'$lookup': {\n",
        "        'from': 'tokens',\n",
        "        'localField': 'tokens',\n",
        "        'foreignField': '_id',\n",
        "        'as': 'tokens'\n",
        "    }},\n",
        "    {'$project': {\n",
        "        'units': True,\n",
        "        'score': True,\n",
        "        'tokens': '$tokens.feature_set'\n",
        "    }},\n",
        "    {'$lookup': {\n",
        "        'from': 'feature_sets',\n",
        "        'localField': 'tokens',\n",
        "        'foreignField': '_id',\n",
        "        'as': 'tokens'\n",
        "    }}\n",
        "])\n",
        "\n",
        "print('\\n')\n",
        "print('{}{}'.format('Score'.ljust(15), 'Match Tokens'.ljust(15)))\n",
        "print('{}{}'.format('-----'.ljust(15), '------------'.ljust(15)))\n",
        "for m in matches:\n",
        "    print('{}{}'.format(('%.3f'%(m.score)).ljust(15), ', '.join(list(set([t['form'] for t in m.tokens])))))\n",
        "    print('{} {} {}: {}'.format(match_texts[0].author, match_texts[0].title, m.units[0]['tags'], ''.join([t['display'] for t in m.units[0]['tokens']])))\n",
        "    print('{} {} {}: {}'.format(match_texts[1].author, match_texts[1].title, m.units[1]['tags'], ''.join([t['display'] for t in m.units[1]['tokens']])))\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1NjY6B7fN_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}