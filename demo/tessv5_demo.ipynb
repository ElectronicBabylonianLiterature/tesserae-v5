{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "tessv5_demo.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPoJ48OmfN-Z",
        "colab_type": "text"
      },
      "source": [
        "# Tesserae v5 Demo\n",
        "\n",
        "This demo will go over the basics of Tesserae v5 development up through February 5, 2019."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VebVIbB6sOy4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "373e32b2-6a3f-4f60-fa98-0e40a65a7a79"
      },
      "source": [
        "!apt update\n",
        "!apt install -y mongodb\n",
        "!mkdir -p mongodb-data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [1 InRelease 3,626 \u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Waiting for header\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Waiting for header\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Waiting for h\u001b[0m\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Waiting for h\u001b[0m\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\u001b[0m\r                                                                               \rGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\u001b[0m\r                                                                               \rGet:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\u001b[0m\r                                                                               \rGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\u001b[0m\r                                                                               \rGet:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [93.7 kB]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [214 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [41.2 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,003 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [9,282 B]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [869 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [89.0 kB]\n",
            "Get:21 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,845 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [13.6 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [102 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,406 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,302 kB]\n",
            "Get:26 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [890 kB]\n",
            "Fetched 8,151 kB in 4s (1,819 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "77 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libpcap0.8 libstemmer0d libyaml-cpp0.5v5 mongo-tools mongodb-clients\n",
            "  mongodb-server mongodb-server-core\n",
            "The following NEW packages will be installed:\n",
            "  libpcap0.8 libstemmer0d libyaml-cpp0.5v5 mongo-tools mongodb mongodb-clients\n",
            "  mongodb-server mongodb-server-core\n",
            "0 upgraded, 8 newly installed, 0 to remove and 77 not upgraded.\n",
            "Need to get 53.1 MB of archives.\n",
            "After this operation, 215 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpcap0.8 amd64 1.8.1-6ubuntu1.18.04.1 [118 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libstemmer0d amd64 0+svn585-1build1 [62.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libyaml-cpp0.5v5 amd64 0.5.2-4ubuntu1 [150 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mongo-tools amd64 3.6.3-0ubuntu1 [12.3 MB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 mongodb-clients amd64 1:3.6.3-0ubuntu1.1 [20.2 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 mongodb-server-core amd64 1:3.6.3-0ubuntu1.1 [20.3 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 mongodb-server all 1:3.6.3-0ubuntu1.1 [12.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 mongodb amd64 1:3.6.3-0ubuntu1.1 [9,968 B]\n",
            "Fetched 53.1 MB in 4s (13.0 MB/s)\n",
            "Selecting previously unselected package libpcap0.8:amd64.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpcap0.8_1.8.1-6ubuntu1.18.04.1_amd64.deb ...\n",
            "Unpacking libpcap0.8:amd64 (1.8.1-6ubuntu1.18.04.1) ...\n",
            "Selecting previously unselected package libstemmer0d:amd64.\n",
            "Preparing to unpack .../1-libstemmer0d_0+svn585-1build1_amd64.deb ...\n",
            "Unpacking libstemmer0d:amd64 (0+svn585-1build1) ...\n",
            "Selecting previously unselected package libyaml-cpp0.5v5:amd64.\n",
            "Preparing to unpack .../2-libyaml-cpp0.5v5_0.5.2-4ubuntu1_amd64.deb ...\n",
            "Unpacking libyaml-cpp0.5v5:amd64 (0.5.2-4ubuntu1) ...\n",
            "Selecting previously unselected package mongo-tools.\n",
            "Preparing to unpack .../3-mongo-tools_3.6.3-0ubuntu1_amd64.deb ...\n",
            "Unpacking mongo-tools (3.6.3-0ubuntu1) ...\n",
            "Selecting previously unselected package mongodb-clients.\n",
            "Preparing to unpack .../4-mongodb-clients_1%3a3.6.3-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking mongodb-clients (1:3.6.3-0ubuntu1.1) ...\n",
            "Selecting previously unselected package mongodb-server-core.\n",
            "Preparing to unpack .../5-mongodb-server-core_1%3a3.6.3-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking mongodb-server-core (1:3.6.3-0ubuntu1.1) ...\n",
            "Selecting previously unselected package mongodb-server.\n",
            "Preparing to unpack .../6-mongodb-server_1%3a3.6.3-0ubuntu1.1_all.deb ...\n",
            "Unpacking mongodb-server (1:3.6.3-0ubuntu1.1) ...\n",
            "Selecting previously unselected package mongodb.\n",
            "Preparing to unpack .../7-mongodb_1%3a3.6.3-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking mongodb (1:3.6.3-0ubuntu1.1) ...\n",
            "Setting up libstemmer0d:amd64 (0+svn585-1build1) ...\n",
            "Setting up libyaml-cpp0.5v5:amd64 (0.5.2-4ubuntu1) ...\n",
            "Setting up mongodb-server-core (1:3.6.3-0ubuntu1.1) ...\n",
            "Setting up libpcap0.8:amd64 (1.8.1-6ubuntu1.18.04.1) ...\n",
            "Setting up mongodb-clients (1:3.6.3-0ubuntu1.1) ...\n",
            "Setting up mongodb-server (1:3.6.3-0ubuntu1.1) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/mongodb.service â†’ /lib/systemd/system/mongodb.service.\n",
            "Setting up mongo-tools (3.6.3-0ubuntu1) ...\n",
            "Setting up mongodb (1:3.6.3-0ubuntu1.1) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.41) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYNtsiNrsjCr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "35cda726-5169-47c6-a009-856a59bb0796"
      },
      "source": [
        "!mongod --dbpath ./mongodb-data --fork --logpath ./mongod.log"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "about to fork child process, waiting until server is ready for connections.\n",
            "forked process: 1417\n",
            "child process started successfully, parent exiting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTHNox59fPef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "24f7e172-8dfc-4c3c-fc2c-cd38cd58f303"
      },
      "source": [
        "!pip install git+https://github.com/ElectronicBabylonianLiterature/tesserae-v5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ElectronicBabylonianLiterature/tesserae-v5\n",
            "  Cloning https://github.com/ElectronicBabylonianLiterature/tesserae-v5 to /tmp/pip-req-build-cz7z5k4k\n",
            "  Running command git clone -q https://github.com/ElectronicBabylonianLiterature/tesserae-v5 /tmp/pip-req-build-cz7z5k4k\n",
            "Requirement already satisfied (use --upgrade to upgrade): tesserae==0.1a1 from git+https://github.com/ElectronicBabylonianLiterature/tesserae-v5 in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: cltk>=0.1.83 in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (0.1.121)\n",
            "Requirement already satisfied: nltk>=3.2.5 in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (3.2.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (1.18.5)\n",
            "Requirement already satisfied: pymongo>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (3.10.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (4.41.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.6/dist-packages (from tesserae==0.1a1) (5.5.0)\n",
            "Requirement already satisfied: python-crfsuite in /usr/local/lib/python3.6/dist-packages (from cltk>=0.1.83->tesserae==0.1a1) (0.9.7)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from cltk>=0.1.83->tesserae==0.1a1) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from cltk>=0.1.83->tesserae==0.1a1) (3.13)\n",
            "Requirement already satisfied: whoosh in /usr/local/lib/python3.6/dist-packages (from cltk>=0.1.83->tesserae==0.1a1) (2.7.4)\n",
            "Requirement already satisfied: pyuca in /usr/local/lib/python3.6/dist-packages (from cltk>=0.1.83->tesserae==0.1a1) (1.2)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.6/dist-packages (from cltk>=0.1.83->tesserae==0.1a1) (3.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.5->tesserae==0.1a1) (1.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from gitpython->cltk>=0.1.83->tesserae==0.1a1) (4.0.5)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->gitpython->cltk>=0.1.83->tesserae==0.1a1) (3.0.4)\n",
            "Building wheels for collected packages: tesserae\n",
            "  Building wheel for tesserae (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tesserae: filename=tesserae-0.1a1-cp36-none-any.whl size=61754 sha256=e47c24682583861ba345b1f11b47210156524ccc5669c5b482c271ed4fb2ccb7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-648mindw/wheels/15/12/9b/1a9461e40ceddee12498dee0f289daa405301c5f616adf3a1c\n",
            "Successfully built tesserae\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIR-RVZbfN-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3389bf44-3026-4a65-eec9-d325bb2a5265"
      },
      "source": [
        "import json\n",
        "\n",
        "from tesserae.db import TessMongoConnection\n",
        "from tesserae.db.entities import Match, Text, Token, Unit\n",
        "from tesserae.utils import TessFile\n",
        "from tesserae.tokenizers import GreekTokenizer, LatinTokenizer\n",
        "from tesserae.unitizer import Unitizer\n",
        "from tesserae.matchers import DefaultMatcher\n",
        "from tesserae.matchers.sparse_encoding import SparseMatrixSearch\n",
        "\n",
        "# Set up the connection and clean up the database\n",
        "connection = TessMongoConnection('127.0.0.1', 27017, None, None, 'tesstest')\n",
        "\n",
        "# Clean up the previous demo\n",
        "connection.connection['feature_sets'].delete_many({})\n",
        "connection.connection['feature']\n",
        "connection.connection['frequencies'].delete_many({})\n",
        "connection.connection['matches'].delete_many({})\n",
        "connection.connection['match_sets'].delete_many({})\n",
        "connection.connection['texts'].delete_many({})\n",
        "connection.connection['tokens'].delete_many({})\n",
        "connection.connection['units'].delete_many({})"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pymongo.results.DeleteResult at 0x7f446552cac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noYHkyCrfN-f",
        "colab_type": "text"
      },
      "source": [
        "## Loading and Storing New Texts\n",
        "\n",
        "The Tesserae database catalogs metadata, including the title, author, and year published, as well as integrity information like filepath, MD5 hash, and CTS URN.\n",
        "\n",
        "We start by loading in some metadata from `text_metadata.json`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNGacqEVfN-g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "b49050c4-2e55-4c53-9ce2-0bf5423d9eda"
      },
      "source": [
        "with open('text_metadata.json', 'r') as f:\n",
        "    text_meta = json.load(f)\n",
        "\n",
        "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
        "print('{}{}{}{}'.format('-----'.ljust(15), '------'.ljust(15), '--------'.ljust(15), '----'))\n",
        "for t in text_meta:\n",
        "    print('{}{}{}{}'.format(t['title'].ljust(15), t['author'].ljust(15), t['language'].ljust(15), str(t['year']).ljust(15)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7de659285304>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text_metadata.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtext_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}{}{}{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Author'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Language'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}{}{}{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-----'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'------'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--------'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'----'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'text_metadata.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGn9rNDmfN-j",
        "colab_type": "text"
      },
      "source": [
        "Then insert the new texts with `TessMongoConnection.insert` after converting the raw JSON to Tesserae `Text` entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEjoXuPcfN-j",
        "colab_type": "code",
        "colab": {},
        "outputId": "e06d243e-e0d1-407a-a4d0-acfe348bf899"
      },
      "source": [
        "texts = []\n",
        "for t in text_meta:\n",
        "    texts.append(Text.json_decode(t))\n",
        "result = connection.insert(texts)\n",
        "print('Inserted {} texts.'.format(len(result.inserted_ids)))\n",
        "print(result.inserted_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inserted 4 texts.\n",
            "[ObjectId('5cd5d8a34273852ca631fafe'), ObjectId('5cd5d8a34273852ca631faff'), ObjectId('5cd5d8a34273852ca631fb00'), ObjectId('5cd5d8a34273852ca631fb01')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MUtvQfXfN-p",
        "colab_type": "text"
      },
      "source": [
        "We can retrieve the inserted texts with `TessMongoConnection.find`. These texts will be converted to objects representing the database entries. The returned text list can be filtered by any valid field in the text database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6S1JGudfN-q",
        "colab_type": "code",
        "colab": {},
        "outputId": "bc4bbabf-5ef6-4c53-891f-2fd895a7f5cf"
      },
      "source": [
        "texts = connection.find('texts', _id=result.inserted_ids)\n",
        "\n",
        "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
        "for t in texts:\n",
        "    print('{}{}{}{}'.format(t.title.ljust(15), t.author.ljust(15), t.language.ljust(15), t.year))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title          Author         Language       Year\n",
            "aeneid         vergil         latin          19\n",
            "de oratore     cicero         latin          38\n",
            "heracles       euripides      greek          -416\n",
            "epistles       plato          greek          -280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U4h9b7RfN-s",
        "colab_type": "text"
      },
      "source": [
        "## Loading .tess Files\n",
        "\n",
        "Text metadata includes the path to the .tess file on the local filesystem. Using a Text retrieved from the database, the file can be loaded for further processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jc_ruUyfN-s",
        "colab_type": "code",
        "colab": {},
        "outputId": "4419042e-3098-4b85-c540-fc5a5fb90585"
      },
      "source": [
        "tessfile = TessFile(texts[0].path, metadata=texts[0])\n",
        "\n",
        "print(tessfile.path)\n",
        "print(len(tessfile))\n",
        "print(tessfile[270])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "la/vergil.aeneid.tess\n",
            "9908\n",
            "<verg. aen. 1.271>\ttransferet, et longam multa vi muniet Albam.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyU-fhZmfN-v",
        "colab_type": "text"
      },
      "source": [
        "We can iterate through the file line-by-line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT5jDyUTfN-v",
        "colab_type": "code",
        "colab": {},
        "outputId": "c7bd8c07-8d50-4d93-d52d-73927ac09f3b"
      },
      "source": [
        "lines = tessfile.readlines()\n",
        "for i in range(10):\n",
        "    print(next(lines))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<verg. aen. 1.1>\tArma virumque cano, Troiae qui primus ab oris\n",
            "\n",
            "<verg. aen. 1.2>\tItaliam, fato profugus, Laviniaque venit\n",
            "\n",
            "<verg. aen. 1.3>\tlitora, multum ille et terris iactatus et alto\n",
            "\n",
            "<verg. aen. 1.4>\tvi superum saevae memorem Iunonis ob iram;\n",
            "\n",
            "<verg. aen. 1.5>\tmulta quoque et bello passus, dum conderet urbem,\n",
            "\n",
            "<verg. aen. 1.6>\tinferretque deos Latio, genus unde Latinum,\n",
            "\n",
            "<verg. aen. 1.7>\tAlbanique patres, atque altae moenia Romae.\n",
            "\n",
            "<verg. aen. 1.8>\tMusa, mihi causas memora, quo numine laeso,\n",
            "\n",
            "<verg. aen. 1.9>\tquidve dolens, regina deum tot volvere casus\n",
            "\n",
            "<verg. aen. 1.10>\tinsignem pietate virum, tot adire labores\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehzPA8bfN-y",
        "colab_type": "text"
      },
      "source": [
        "We can also iterate token-by-token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMlabOnkfN-y",
        "colab_type": "code",
        "colab": {},
        "outputId": "e9e15758-f4dc-405a-b90c-8ac8f028a5a5"
      },
      "source": [
        "tokens = tessfile.read_tokens()\n",
        "for i in range(10):\n",
        "    print(next(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arma\n",
            "virumque\n",
            "cano,\n",
            "Troiae\n",
            "qui\n",
            "primus\n",
            "ab\n",
            "oris\n",
            "Italiam,\n",
            "fato\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3skP5flfN-1",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing a Text\n",
        "\n",
        "Texts can be tokenized with `tesserae.tokenizers` objects. These objects are designed to normalize and compute features for tokens of a specific language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08wu-WZrfN-1",
        "colab_type": "code",
        "colab": {},
        "outputId": "f823e1b5-cbb4-4b60-aa92-4e984b14279c"
      },
      "source": [
        "tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
        "\n",
        "tokens, tags, features = tokenizer.tokenize2(tessfile.read(), text=tessfile.metadata)\n",
        "\n",
        "print(len(tokens), len(tags), len(features))\n",
        "\n",
        "print('{}{}{}{}'.format('Raw'.ljust(15), 'Normalized'.ljust(15), 'Lemmata'.ljust(20), 'Frequency'))\n",
        "print('{}{}{}{}'.format('---'.ljust(15), '----------'.ljust(15), '-------'.ljust(20), '---------'))\n",
        "for i in range(20):\n",
        "    if len(tokens[i].features):\n",
        "        print('{}{}{}{}'.format(tokens[i].display.ljust(15),\n",
        "                              str(tokens[i].features['form'].token).ljust(20),\n",
        "                              str(tokens[i].features['lemmata'][0].token).ljust(20),\n",
        "                              list(tokens[i].features['form'].frequencies.values())[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MemoryError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f41e23837c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGreekTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtessfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'greek'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mLatinTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtessfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtessfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tessv5/lib/python3.6/site-packages/tesserae/tokenizers/base.py\u001b[0m in \u001b[0;36mtokenize2\u001b[0;34m(self, raw, record, text)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mfeature_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                         \u001b[0mfeature_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0mnorm_i\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMemoryError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0k_igU5fN-4",
        "colab_type": "text"
      },
      "source": [
        "Processed tokens can then be stored in and retrieved from the database, similar to text metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urZTBdFzfN-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = connection.insert(features)\n",
        "print('Inserted {} feature entities out of {}'.format(len(result.inserted_ids), len(features)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuLJFTw2fN-6",
        "colab_type": "text"
      },
      "source": [
        "## Unitizing a Text\n",
        "\n",
        "Texts can be unitized into lines and phrases, and the intertext matches are found between units of text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oOUmwI7fN-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unitizing lines of a poem\n",
        "unitizer = Unitizer()\n",
        "lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
        "\n",
        "print('Lines\\n-----')\n",
        "for line in lines[:20]:\n",
        "        print(''.join([str(line.tags), ': '] + [t.display for t in line.tokens]))\n",
        "        \n",
        "print('\\n\\nPhrases\\n-------')\n",
        "for phrase in phrases[:20]:\n",
        "        print(''.join([str(phrase.tags), ': '] + [t.display for t in phrase.tokens]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwKa94FEfN-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unitizing phrases of a poem or prose\n",
        "result = connection.insert(lines + phrases)\n",
        "print('Inserted {} units out of {}.'.format(len(result.inserted_ids), len(lines + phrases)))\n",
        "\n",
        "\n",
        "result = connection.insert(tokens)\n",
        "print('Inserted {} tokens out of {}.'.format(len(result.inserted_ids), len(tokens)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOdy-Up5fN-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for text in texts[1:]:\n",
        "    tessfile = TessFile(text.path, metadata=text)\n",
        "    tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
        "\n",
        "    \n",
        "    tokens, tags, frequencies, feature_sets = tokenizer.tokenize(tessfile.read(), text=tessfile.metadata)\n",
        "        \n",
        "    tokens = tokenizer.tokens\n",
        "    result = connection.insert(feature_sets)\n",
        "    result = connection.insert(frequencies)\n",
        "    \n",
        "    unitizer = Unitizer()\n",
        "    lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
        "    result = connection.insert(lines + phrases)\n",
        "    \n",
        "    result = connection.insert(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi09bzDPfN_C",
        "colab_type": "text"
      },
      "source": [
        "## Matching\n",
        "\n",
        "Once the Texts, Tokens, and Units are in the database, we can then find intertext matches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEJyHxDCfN_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "matcher = DefaultMatcher(connection)\n",
        "match_texts = [t for t in texts if t.language == 'greek']\n",
        "\n",
        "start = time.time()\n",
        "matches, match_set = matcher.match(match_texts, 'phrase', 'form', distance_metric='span', stopwords=20, max_distance=10)\n",
        "print(\"Completed matching in {0:.2f}s\".format(time.time() - start))\n",
        "\n",
        "matches.sort(key=lambda x: x.score, reverse=True)\n",
        "\n",
        "# result = connection.insert(match_set)\n",
        "# print('Inserted {} match set entities out of {}'.format(len(result.inserted_ids), 1))\n",
        "result = connection.insert(matches)\n",
        "print('Inserted {} match entities out of {}'.format(len(result.inserted_ids), len(matches)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzLx0OqgfN_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matches = connection.aggregate('matches', [\n",
        "    {'$match': {'match_set': match_set.id}},\n",
        "    {'$sort': {'score': -1}},\n",
        "    {'$limit': 20},\n",
        "    {'$lookup': {\n",
        "        'from': 'units',\n",
        "        'let': {'m_units': '$units'},\n",
        "        'pipeline': [\n",
        "            {'$match': {'$expr': {'$in': ['$_id', '$$m_units']}}},\n",
        "            {'$lookup': {\n",
        "                'from': 'tokens',\n",
        "                'localField': '_id',\n",
        "                'foreignField': 'phrase',\n",
        "                'as': 'tokens'\n",
        "            }},\n",
        "            {'$sort': {'index': 1}}\n",
        "        ],\n",
        "        'as': 'units'\n",
        "    }},\n",
        "    {'$lookup': {\n",
        "        'from': 'tokens',\n",
        "        'localField': 'tokens',\n",
        "        'foreignField': '_id',\n",
        "        'as': 'tokens'\n",
        "    }},\n",
        "    {'$project': {\n",
        "        'units': True,\n",
        "        'score': True,\n",
        "        'tokens': '$tokens.feature_set'\n",
        "    }},\n",
        "    {'$lookup': {\n",
        "        'from': 'feature_sets',\n",
        "        'localField': 'tokens',\n",
        "        'foreignField': '_id',\n",
        "        'as': 'tokens'\n",
        "    }}\n",
        "])\n",
        "\n",
        "print('\\n')\n",
        "print('{}{}'.format('Score'.ljust(15), 'Match Tokens'.ljust(15)))\n",
        "print('{}{}'.format('-----'.ljust(15), '------------'.ljust(15)))\n",
        "for m in matches:\n",
        "    print('{}{}'.format(('%.3f'%(m.score)).ljust(15), ', '.join(list(set([t['form'] for t in m.tokens])))))\n",
        "    print('{} {} {}: {}'.format(match_texts[0].author, match_texts[0].title, m.units[0]['tags'], ''.join([t['display'] for t in m.units[0]['tokens']])))\n",
        "    print('{} {} {}: {}'.format(match_texts[1].author, match_texts[1].title, m.units[1]['tags'], ''.join([t['display'] for t in m.units[1]['tokens']])))\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1NjY6B7fN_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}