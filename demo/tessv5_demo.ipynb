{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesserae v5 Demo\n",
    "\n",
    "This demo will go over the basics of Tesserae v5 development up through February 5, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tesserae.db import TessMongoConnection\n",
    "from tesserae.db.entities import Frequency, Match, Text, Token, Unit\n",
    "from tesserae.utils import TessFile\n",
    "from tesserae.tokenizers import GreekTokenizer, LatinTokenizer\n",
    "from tesserae.unitizer import Unitizer\n",
    "from tesserae.matchers import AggregationMatcher\n",
    "\n",
    "# Set up the connection and clean up the database\n",
    "connection = TessMongoConnection('127.0.0.1', 27017, None, None, 'tesstest')\n",
    "\n",
    "# Clean up the previous demo\n",
    "connection.connection['feature_sets'].delete_many({})\n",
    "connection.connection['frequencies'].delete_many({})\n",
    "connection.connection['matches'].delete_many({})\n",
    "connection.connection['match_sets'].delete_many({})\n",
    "connection.connection['texts'].delete_many({})\n",
    "connection.connection['tokens'].delete_many({})\n",
    "connection.connection['units'].delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Storing New Texts\n",
    "\n",
    "The Tesserae database catalogs metadata, including the title, author, and year published, as well as integrity information like filepath, MD5 hash, and CTS URN.\n",
    "\n",
    "We start by loading in some metadata from `text_metadata.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_metadata.json', 'r') as f:\n",
    "    text_meta = json.load(f)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "print('{}{}{}{}'.format('-----'.ljust(15), '------'.ljust(15), '--------'.ljust(15), '----'))\n",
    "for t in text_meta:\n",
    "    print('{}{}{}{}'.format(t['title'].ljust(15), t['author'].ljust(15), t['language'].ljust(15), str(t['year']).ljust(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then insert the new texts with `TessMongoConnection.insert` after converting the raw JSON to Tesserae `Text` entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for t in text_meta:\n",
    "    texts.append(Text.json_decode(t))\n",
    "result = connection.insert(texts)\n",
    "print('Inserted {} texts.'.format(len(result.inserted_ids)))\n",
    "print(result.inserted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the inserted texts with `TessMongoConnection.find`. These texts will be converted to objects representing the database entries. The returned text list can be filtered by any valid field in the text database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = connection.find('texts', _id=result.inserted_ids)\n",
    "\n",
    "print('{}{}{}{}'.format('Title'.ljust(15), 'Author'.ljust(15), 'Language'.ljust(15), 'Year'))\n",
    "for t in texts:\n",
    "    print('{}{}{}{}'.format(t.title.ljust(15), t.author.ljust(15), t.language.ljust(15), t.year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading .tess Files\n",
    "\n",
    "Text metadata includes the path to the .tess file on the local filesystem. Using a Text retrieved from the database, the file can be loaded for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessfile = TessFile(texts[0].path, metadata=texts[0])\n",
    "\n",
    "print(tessfile.path)\n",
    "print(len(tessfile))\n",
    "print(tessfile[270])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the file line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = tessfile.readlines()\n",
    "for i in range(10):\n",
    "    print(next(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also iterate token-by-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tessfile.read_tokens()\n",
    "for i in range(10):\n",
    "    print(next(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing a Text\n",
    "\n",
    "Texts can be tokenized with `tesserae.tokenizers` objects. These objects are designed to normalize and compute features for tokens of a specific language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
    "\n",
    "tokens, tags, frequencies, feature_sets = tokenizer.tokenize(tessfile.read(), text=tessfile.metadata)\n",
    "\n",
    "tokens = tokenizer.tokens\n",
    "print(len(tokens), len(frequencies), len(feature_sets))\n",
    "\n",
    "print('{}{}{}{}'.format('Raw'.ljust(15), 'Normalized'.ljust(15), 'Lemmata'.ljust(20), 'Frequency'))\n",
    "print('{}{}{}{}'.format('---'.ljust(15), '----------'.ljust(15), '-------'.ljust(20), '---------'))\n",
    "for i in range(20):\n",
    "    if tokenizer.tokens[i].feature_set is not None and not isinstance(tokenizer.tokens[i].feature_set, str):\n",
    "        print('{}{}{}{}'.format(tokenizer.tokens[i].display.ljust(15),\n",
    "                              str(tokenizer.tokens[i].feature_set.form).ljust(20),\n",
    "                              str(tokenizer.tokens[i].feature_set.lemmata).ljust(20),\n",
    "                              tokenizer.tokens[i].frequency.frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed tokens can then be stored in and retrieved from the database, similar to text metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = connection.insert(feature_sets)\n",
    "print('Inserted {} feature set entities out of {}'.format(len(result.inserted_ids), len(feature_sets)))\n",
    "\n",
    "result = connection.insert(frequencies)\n",
    "print('Inserted {} frequency entities out of {}'.format(len(result.inserted_ids), len(frequencies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitizing a Text\n",
    "\n",
    "Texts can be unitized into lines and phrases, and the intertext matches are found between units of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitizing lines of a poem\n",
    "unitizer = Unitizer()\n",
    "lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
    "\n",
    "print('Lines\\n-----')\n",
    "for line in lines[:20]:\n",
    "        print(''.join([str(line.tags), ': '] + [t.display for t in line.tokens]))\n",
    "        \n",
    "print('\\n\\nPhrases\\n-------')\n",
    "for phrase in phrases[:20]:\n",
    "        print(''.join([str(phrase.tags), ': '] + [t.display for t in phrase.tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unitizing phrases of a poem or prose\n",
    "result = connection.insert(lines + phrases)\n",
    "print('Inserted {} units out of {}.'.format(len(result.inserted_ids), len(lines + phrases)))\n",
    "\n",
    "\n",
    "result = connection.insert(tokens)\n",
    "print('Inserted {} tokens out of {}.'.format(len(result.inserted_ids), len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts[1:]:\n",
    "    tessfile = TessFile(text.path, metadata=text)\n",
    "    tokenizer = GreekTokenizer(connection) if tessfile.metadata.language == 'greek' else LatinTokenizer(connection)\n",
    "\n",
    "    \n",
    "    tokens, tags, frequencies, feature_sets = tokenizer.tokenize(tessfile.read(), text=tessfile.metadata)\n",
    "        \n",
    "    tokens = tokenizer.tokens\n",
    "    result = connection.insert(feature_sets)\n",
    "    result = connection.insert(frequencies)\n",
    "    \n",
    "    unitizer = Unitizer()\n",
    "    lines, phrases = unitizer.unitize(tokens, tags, tessfile.metadata)\n",
    "    result = connection.insert(lines + phrases)\n",
    "    \n",
    "    result = connection.insert(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching\n",
    "\n",
    "Once the Texts, Tokens, and Units are in the database, we can then find intertext matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "matcher = AggregationMatcher(connection)\n",
    "match_texts = [t for t in texts if t.language == 'greek']\n",
    "\n",
    "start = time.time()\n",
    "matches, match_set = matcher.match(match_texts, 'phrase', 'form', distance_metric='span', stopwords=20, max_distance=10)\n",
    "print(\"Completed matching in {0:.2f}s\".format(time.time() - start))\n",
    "\n",
    "matches.sort(key=lambda x: x.score, reverse=True)\n",
    "\n",
    "# result = connection.insert(match_set)\n",
    "# print('Inserted {} match set entities out of {}'.format(len(result.inserted_ids), 1))\n",
    "result = connection.insert(matches)\n",
    "print('Inserted {} match entities out of {}'.format(len(result.inserted_ids), len(matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = connection.aggregate('matches', [\n",
    "    {'$match': {'match_set': match_set.id}},\n",
    "    {'$sort': {'score': -1}},\n",
    "    {'$limit': 20},\n",
    "    {'$lookup': {\n",
    "        'from': 'units',\n",
    "        'let': {'m_units': '$units'},\n",
    "        'pipeline': [\n",
    "            {'$match': {'$expr': {'$in': ['$_id', '$$m_units']}}},\n",
    "            {'$lookup': {\n",
    "                'from': 'tokens',\n",
    "                'localField': '_id',\n",
    "                'foreignField': 'phrase',\n",
    "                'as': 'tokens'\n",
    "            }},\n",
    "            {'$sort': {'index': 1}}\n",
    "        ],\n",
    "        'as': 'units'\n",
    "    }},\n",
    "    {'$lookup': {\n",
    "        'from': 'tokens',\n",
    "        'localField': 'tokens',\n",
    "        'foreignField': '_id',\n",
    "        'as': 'tokens'\n",
    "    }},\n",
    "    {'$project': {\n",
    "        'units': True,\n",
    "        'score': True,\n",
    "        'tokens': '$tokens.feature_set'\n",
    "    }},\n",
    "    {'$lookup': {\n",
    "        'from': 'feature_sets',\n",
    "        'localField': 'tokens',\n",
    "        'foreignField': '_id',\n",
    "        'as': 'tokens'\n",
    "    }}\n",
    "])\n",
    "\n",
    "print('\\n')\n",
    "print('{}{}'.format('Score'.ljust(15), 'Match Tokens'.ljust(15)))\n",
    "print('{}{}'.format('-----'.ljust(15), '------------'.ljust(15)))\n",
    "for m in matches:\n",
    "    print('{}{}'.format(('%.3f'%(m.score)).ljust(15), ', '.join(list(set([t['form'] for t in m.tokens])))))\n",
    "    print('{} {} {}: {}'.format(match_texts[0].author, match_texts[0].title, m.units[0]['tags'], ''.join([t['display'] for t in m.units[0]['tokens']])))\n",
    "    print('{} {} {}: {}'.format(match_texts[1].author, match_texts[1].title, m.units[1]['tags'], ''.join([t['display'] for t in m.units[1]['tokens']])))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tess-env",
   "language": "python",
   "name": "tess-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
